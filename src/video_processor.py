# -*- coding: utf-8 -*-
import os
import sys
import tempfile
import subprocess
import glob
import time

# Force UTF-8 encoding for stdout
import codecs
sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())
# Set environment variable for proper encoding
os.environ['PYTHONIOENCODING'] = 'utf-8'



from moviepy.editor import *
import random
import numpy as np
import datetime
import cv2
from PIL import Image
import warnings
import subprocess
import time
import tempfile
import urllib.parse
from pathlib import Path
import glob
warnings.filterwarnings('ignore')

# ================= AUDIO DOWNLOAD FUNCTIONS =================
def download_audio_from_url(url, output_folder=None):
    """
    Download audio t·ª´ YouTube, SoundCloud, Spotify URLs
    Args:
        url: URL c·ªßa nh·∫°c
        output_folder: Th∆∞ m·ª•c l∆∞u file (m·∫∑c ƒë·ªãnh l√† temp)
    Returns:
        ƒê∆∞·ªùng d·∫´n file audio ƒë√£ download ho·∫∑c None n·∫øu l·ªói
    """
    try:
        if not output_folder:
            output_folder = tempfile.gettempdir()
        
        # T·∫°o t√™n file output d·ª±a tr√™n timestamp
        import time
        timestamp = int(time.time())
        output_template = os.path.join(output_folder, f"downloaded_audio_{timestamp}.%(ext)s")
        
        print(f"üéµ Downloading audio from: {url}")
        print(f"üìÅ Output folder: {output_folder}")
        
        # C·∫•u h√¨nh yt-dlp command (d√πng python -m yt_dlp ƒë·ªÉ ch·∫Øc ch·∫Øn ch·∫°y ƒë∆∞·ª£c)
        cmd = [
            sys.executable, "-m", "yt_dlp",
            "--extract-audio",
            "--audio-format", "mp3",
            "--audio-quality", "192K",
            "--output", output_template,
            "--no-playlist",  # Ch·ªâ download 1 video/track
            "--quiet",        # Gi·∫£m output verbose
            url
        ]
        print(f"üîß Running command: {' '.join(cmd)}")
        # Ch·∫°y yt-dlp qua python module
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        if result.returncode == 0:
            # T√¨m file ƒë√£ download
            mp3_file = os.path.join(output_folder, f"downloaded_audio_{timestamp}.mp3")
            if os.path.exists(mp3_file):
                print(f"‚úÖ Downloaded successfully: {mp3_file}")
                return mp3_file
            else:
                # T√¨m file v·ªõi pattern kh√°c
                pattern = os.path.join(output_folder, f"downloaded_audio_{timestamp}.*")
                files = glob.glob(pattern)
                if files:
                    downloaded_file = files[0]
                    print(f"‚úÖ Downloaded successfully: {downloaded_file}")
                    return downloaded_file
                else:
                    print(f"‚ùå Download completed but file not found")
                    return None
        else:
            print(f"‚ùå yt-dlp error: {result.stderr}")
            return None
            
    except subprocess.TimeoutExpired:
        print(f"‚ùå Download timeout after 120 seconds")
        return None
    except Exception as e:
        print(f"‚ùå Download error: {str(e)}")
        return None

def is_supported_music_url(url):
    """
    Ki·ªÉm tra xem URL c√≥ ƒë∆∞·ª£c h·ªó tr·ª£ kh√¥ng
    """
    supported_domains = [
        'youtube.com', 'youtu.be', 'm.youtube.com',
        'soundcloud.com', 'on.soundcloud.com',
        'spotify.com', 'open.spotify.com',
        'bandcamp.com'
    ]
    
    try:
        parsed = urllib.parse.urlparse(url)
        domain = parsed.netloc.lower()
        
        # Lo·∫°i b·ªè www. prefix
        if domain.startswith('www.'):
            domain = domain[4:]
            
        return any(supported_domain in domain for supported_domain in supported_domains)
    except:
        return False

# ================= TIMER & LOGGING SYSTEM =================
class VideoTimer:
    """H·ªá th·ªëng timing cho video processing"""
    def __init__(self):
        self.start_time = None
        self.phase_times = {}
        self.current_phase = None
        
    def start(self, operation="Video Processing"):
        """B·∫Øt ƒë·∫ßu ƒë·∫øm th·ªùi gian"""
        self.start_time = time.time()
        current_time = datetime.datetime.now().strftime("%H:%M:%S")
        print(f"\n‚è∞ [{current_time}] üöÄ B·∫ÆT ƒê·∫¶U: {operation}")
        print("="*60)
        
    def phase_start(self, phase_name):
        """B·∫Øt ƒë·∫ßu m·ªôt phase m·ªõi"""
        current_time = datetime.datetime.now().strftime("%H:%M:%S")
        phase_start_time = time.time()
        
        if self.current_phase:
            # K·∫øt th√∫c phase tr∆∞·ªõc
            elapsed = phase_start_time - self.phase_times[self.current_phase]
            print(f"‚è±Ô∏è  [{current_time}] ‚úÖ Ho√†n th√†nh: {self.current_phase} ({elapsed:.1f}s)")
            
        self.current_phase = phase_name
        self.phase_times[phase_name] = phase_start_time
        print(f"‚è±Ô∏è  [{current_time}] üîÑ B·∫Øt ƒë·∫ßu: {phase_name}")
        
    def phase_update(self, message):
        """C·∫≠p nh·∫≠t ti·∫øn tr√¨nh trong phase"""
        current_time = datetime.datetime.now().strftime("%H:%M:%S")
        if self.current_phase:
            elapsed = time.time() - self.phase_times[self.current_phase]
            print(f"   [{current_time}] üìù {message} (+{elapsed:.1f}s)")
        else:
            print(f"   [{current_time}] üìù {message}")
            
    def finish(self):
        """K·∫øt th√∫c v√† b√°o c√°o t·ªïng th·ªùi gian"""
        end_time = time.time()
        current_time = datetime.datetime.now().strftime("%H:%M:%S")
        
        if self.current_phase:
            # K·∫øt th√∫c phase cu·ªëi
            elapsed = end_time - self.phase_times[self.current_phase]
            print(f"‚è±Ô∏è  [{current_time}] ‚úÖ Ho√†n th√†nh: {self.current_phase} ({elapsed:.1f}s)")
            
        total_time = end_time - self.start_time
        minutes = int(total_time // 60)
        seconds = total_time % 60
        
        print("="*60)
        print(f"üéâ [{current_time}] ‚úÖ HO√ÄN TH√ÄNH VIDEO!")
        print(f"‚è∞ T·ªîNG TH·ªúI GIAN: {minutes}:{seconds:05.2f} ({total_time:.1f} gi√¢y)")
        print("="*60)
        
        return total_time

# Kh·ªüi t·∫°o timer global
video_timer = VideoTimer()

# ================= GPU DETECTION & SETUP =================
def check_gpu_availability():
    """Ki·ªÉm tra GPU NVIDIA c√≥ s·∫µn v√† CUDA support"""
    try:
        # Ki·ªÉm tra nvidia-smi command
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            # Ki·ªÉm tra FFmpeg c√≥ h·ªó tr·ª£ h264_nvenc kh√¥ng
            ffmpeg_result = subprocess.run(['ffmpeg', '-encoders'], capture_output=True, text=True, timeout=10)
            if 'h264_nvenc' in ffmpeg_result.stdout:
                return True, "NVIDIA GPU detected with h264_nvenc support"
            else:
                return False, "NVIDIA GPU detected but h264_nvenc not available in FFmpeg"
        else:
            return False, "NVIDIA GPU not detected"
    except Exception as e:
        return False, f"GPU check failed: {str(e)}"

def log_gpu_status():
    """Log GPU status v√† khuy·∫øn ngh·ªã v·ªõi performance optimization"""
    gpu_available, gpu_message = check_gpu_availability()
    if gpu_available:
        log(f"üöÄ GPU Status: {gpu_message}")
        log("üí° GPU acceleration ENABLED - render s·∫Ω nhanh h∆°n 5-10x")
        
        # Optimization tips
        try:
            # Get GPU memory info
            result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total,memory.used,memory.free', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                memory_info = result.stdout.strip().split(', ')
                total_mem = int(memory_info[0])
                used_mem = int(memory_info[1])
                free_mem = int(memory_info[2])
                
                log(f"üìä GPU Memory: {free_mem}MB free / {total_mem}MB total")
                
                if free_mem > 3000:  # > 3GB free
                    log("üî• Excellent GPU memory - Maximum performance mode enabled")
                elif free_mem > 1000:  # > 1GB free
                    log("‚úÖ Good GPU memory - High performance mode enabled")
                else:
                    log("‚ö†Ô∏è Limited GPU memory - Standard performance mode")
                    
        except:
            log("üìä GPU Memory check failed - using default settings")
            
        return True
    else:
        log(f"‚ö†Ô∏è GPU Status: {gpu_message}")
        log("üí° GPU acceleration DISABLED - s·ª≠ d·ª•ng CPU encoding")
        return False

def log_cpu_status():
    """Log CPU status v√† performance optimization cho 44 cores"""
    import multiprocessing
    try:
        import psutil
    except ImportError:
        log("‚ö†Ô∏è psutil not available - using basic CPU info")
        psutil = None
    
    try:
        cpu_count = multiprocessing.cpu_count()
        
        if psutil:
            memory_gb = round(psutil.virtual_memory().total / (1024**3))
            memory_available_gb = round(psutil.virtual_memory().available / (1024**3))
            log(f"üöÄ CPU Status: {cpu_count} cores detected, {memory_gb}GB total RAM")
            log(f"üíæ Available RAM: {memory_available_gb}GB / {memory_gb}GB")
        else:
            log(f"üöÄ CPU Status: {cpu_count} cores detected")
        
        log("üí° CPU acceleration ENABLED - t·ªëi ∆∞u cho hi·ªáu su·∫•t cao")
        log("‚ö° Mode: MAXIMUM PERFORMANCE v·ªõi multi-threading t·ªëi ƒëa")
        
        if cpu_count >= 40:
            log("üî• EXCELLENT CPU - 40+ cores detected! Maximum optimization enabled")
        elif cpu_count >= 20:
            log("‚úÖ POWERFUL CPU - 20+ cores detected! High performance mode")
        elif cpu_count >= 8:
            log("üëç GOOD CPU - 8+ cores detected! Standard optimization")
        else:
            log("‚ö†Ô∏è LIMITED CPU - <8 cores detected! Basic optimization")
        
        if psutil:
            if memory_gb >= 64:
                log("üöÄ MASSIVE RAM - 64GB+ detected! Maximum buffer sizes enabled")
            elif memory_gb >= 32:
                log("‚úÖ PLENTY RAM - 32GB+ detected! Large buffer optimization")
            elif memory_gb >= 16:
                log("üëç GOOD RAM - 16GB+ detected! Standard buffers")
            else:
                log("‚ö†Ô∏è LIMITED RAM - <16GB detected! Conservative settings")
        
        log(f"üéØ Optimization: x264 preset '{CPU_PRESET}', CRF {CPU_CRF}, {THREADS} threads")
        log(f"üìä Target quality: Very High (CRF {CPU_CRF}), Bitrate: {CPU_BITRATE}")
        log("üöÄ RENDERING MODE: CPU MAXIMUM PERFORMANCE (GPU disabled)")
        
    except Exception as e:
        log(f"‚ö†Ô∏è CPU status check failed: {str(e)}")
        log("üíª Using default CPU settings")

def optimize_gpu_settings():
    """T·ªëi ∆∞u GPU settings d·ª±a tr√™n hardware"""
    try:
        global GPU_EXTRA_PARAMS
        
        # Check GPU memory for optimization
        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.free', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            free_mem = int(result.stdout.strip())
            
            if free_mem > 3000:  # > 3GB free - Maximum performance
                log("üî• MAXIMUM GPU memory detected - Optimizing for speed")
                # Thay ƒë·ªïi surfaces v√† async_depth cho maximum performance
                for i, param in enumerate(GPU_EXTRA_PARAMS):
                    if param == "-surfaces":
                        GPU_EXTRA_PARAMS[i+1] = "64"  # Maximum surfaces
                    elif param == "-async_depth":
                        GPU_EXTRA_PARAMS[i+1] = "8"   # Maximum async depth
                        
            elif free_mem > 1500:  # > 1.5GB free - High performance  
                log("‚ö° HIGH GPU memory detected - Balanced optimization")
                # Gi·ªØ nguy√™n settings hi·ªán t·∫°i (32 surfaces, 4 async_depth)
                pass
            else:  # Limited memory - Conservative
                log("üîß LIMITED GPU memory - Conservative settings")
                for i, param in enumerate(GPU_EXTRA_PARAMS):
                    if param == "-surfaces":
                        GPU_EXTRA_PARAMS[i+1] = "16"  # Conservative surfaces
                    elif param == "-async_depth":
                        GPU_EXTRA_PARAMS[i+1] = "2"   # Conservative async depth
                
        return True
    except Exception as e:
        log(f"‚ö†Ô∏è GPU optimization failed: {str(e)}")
        return False

# ================= C·∫§U H√åNH THAM S·ªê =================
INPUT_FOLDER = "./photos"       # Th∆∞ m·ª•c ch·ª©a ·∫£nh v√† video
OUTPUT_FOLDER = "."             # Th∆∞ m·ª•c ƒë·∫ßu ra
OUTRO_FOLDER = "./outro"        # üÜï Th∆∞ m·ª•c ch·ª©a video outro

# T·∫°o t√™n file output v·ªõi timestamp (format: hhmm_ddmm.mp4)
timestamp = datetime.datetime.now().strftime("%H%M_%d%m")
OUTPUT_FILENAME = f"{timestamp}.mp4" # T√™n file output v·ªõi timestamp
OUTPUT_VIDEO = OUTPUT_FILENAME   # T√™n file output (t∆∞∆°ng th√≠ch)
MUSIC_FOLDER = "./music"        # Th∆∞ m·ª•c ch·ª©a nh·∫°c
TARGET_DURATION = 90            # T·ªïng th·ªùi l∆∞·ª£ng mong mu·ªën (gi√¢y) - t·ªïng video output
MIN_DURATION = 75               # Th·ªùi l∆∞·ª£ng t·ªëi thi·ªÉu (gi√¢y)
MAX_VIDEO_MATERIALS_DURATION = 52  # T·ªïng th·ªùi l∆∞·ª£ng t·ªëi ƒëa cho video nguy√™n li·ªáu (gi√¢y)

# üÜï ORIENTATION SETTINGS
RESOLUTION_HORIZONTAL = (1280, 720)    # 16:9 720p ngang
RESOLUTION_VERTICAL = (720, 1280)      # 9:16 720p d·ªçc (TikTok/Reel)
RESOLUTION_SQUARE = (720, 720)         # 1:1 720p vu√¥ng (Instagram Post)
RESOLUTION = RESOLUTION_HORIZONTAL     # M·∫∑c ƒë·ªãnh ngang (s·∫Ω ƒë∆∞·ª£c update)

FPS = 30                        # S·ªë khung h√¨nh/gi√¢y (30fps cho smooth)
CODEC = "libx264"               # Video codec
AUDIO_CODEC = "aac"             # Audio codec
THREADS = 44                    # S·ª≠ d·ª•ng t·∫•t c·∫£ 44 cores c·ªßa CPU m·∫°nh m·∫Ω (c·∫≠p nh·∫≠t t·ª´ 22)

# üöÄ CPU ACCELERATION SETTINGS - OPTIMIZED FOR 44 CORES & 128GB RAM
GPU_ENABLED = False             # T·∫Øt GPU acceleration - s·ª≠ d·ª•ng CPU m·∫°nh m·∫Ω
GPU_CODEC = "h264_nvenc"        # NVIDIA hardware encoder (GTX 1660 support)

# ================= MANUAL SELECTION SETTINGS =================
IS_MANUAL_SELECTION = False     # Flag ƒë·ªÉ bi·∫øt user c√≥ t·ª± ch·ªçn file kh√¥ng
MANUAL_SELECTED_FILES = []      # List c√°c file user ƒë√£ ch·ªçn
GPU_PRESET = "p4"               # Balanced preset (p4=medium speed, good quality)
GPU_MULTIPASS = "disabled"      # T·∫Øt multi-pass ƒë·ªÉ tƒÉng t·ªëc
GPU_CRF = 20                    # Quality: 20=good quality nh∆∞ng nhanh h∆°n (thay v√¨ 16)
GPU_BITRATE = "6M"              # Bitrate h·ª£p l√Ω cho 720p (gi·∫£m t·ª´ 10M xu·ªëng 6M)
GPU_EXTRA_PARAMS = [
    "-c:v", "h264_nvenc",       # Video codec
    "-preset", "p4",            # Balanced preset (p4=medium speed, good quality)
    "-tune", "ll",              # Low latency tuning (nhanh h∆°n hq)
    "-profile:v", "main",       # Main profile (nhanh h∆°n high)
    "-pix_fmt", "yuv420p",      # Pixel format (Windows compatibility)
    "-level:v", "4.0",          # H.264 level standard (4.0 thay v√¨ 4.1)
    "-rc:v", "vbr",             # Variable bitrate standard mode (nhanh h∆°n vbr_hq)
    "-cq:v", "20",              # Constant quality 20 (balanced speed/quality)
    "-b:v", "6M",               # Video bitrate h·ª£p l√Ω cho 720p
    "-maxrate:v", "8M",         # Max bitrate th·∫•p h∆°n
    "-bufsize:v", "12M",        # Buffer size nh·ªè h∆°n
    "-spatial_aq", "0",         # T·∫Øt spatial AQ ƒë·ªÉ tƒÉng t·ªëc
    "-temporal_aq", "0",        # T·∫Øt temporal AQ ƒë·ªÉ tƒÉng t·ªëc
    "-rc-lookahead", "8",       # Look-ahead frames th·∫•p ƒë·ªÉ tƒÉng t·ªëc (gi·∫£m t·ª´ 32 xu·ªëng 8)
    "-bf", "0",                 # T·∫Øt B-frames
    "-surfaces", "32",          # GPU surfaces √≠t h∆°n ƒë·ªÉ tƒÉng t·ªëc (gi·∫£m t·ª´ 64 xu·ªëng 32)
    "-async_depth", "4",        # Async processing depth ƒë·ªÉ tƒÉng t·ªëc
    "-c:a", "aac",              # Audio codec
    "-b:a", "192k",             # Audio bitrate standard (gi·∫£m t·ª´ 256k xu·ªëng 192k)
    "-movflags", "+faststart"   # Optimize for web playback
]

# üíª CPU OPTIMIZATION SETTINGS - MAXIMUM PERFORMANCE FOR 44 CORES
CPU_CODEC = "libx264"                   # CPU codec t·ªëi ∆∞u
CPU_PRESET = "faster"                   # Preset nhanh cho CPU m·∫°nh (faster thay v√¨ medium)
CPU_CRF = 18                            # Quality cao cho CPU (18 thay v√¨ 23 m·∫∑c ƒë·ªãnh)
CPU_BITRATE = "12M"                     # Bitrate cao h∆°n GPU v√¨ CPU c√≥ th·ªÉ handle
CPU_EXTRA_PARAMS = [
    "-c:v", "libx264",                  # Video codec t·ªëi ∆∞u CPU
    "-preset", "faster",                # Preset nhanh cho CPU m·∫°nh (44 cores)
    "-crf", "18",                       # Quality cao (18 = very good quality)
    "-profile:v", "high",               # High profile for best quality
    "-level:v", "4.2",                  # Level cao cho high bitrate
    "-pix_fmt", "yuv420p",              # Standard pixel format
    "-x264-params", "threads=44:thread-input=1:thread-lookahead=8", # Maximize thread usage
    "-tune", "film",                    # Optimize for high-quality video content
    "-bf", "3",                         # B-frames for better compression
    "-refs", "5",                       # Reference frames for quality
    "-me_method", "umh",                # Motion estimation for quality
    "-subq", "8",                       # Subpixel motion quality
    "-partitions", "+parti8x8+parti4x4+partp8x8+partb8x8", # All partition types
    "-direct-pred", "3",                # Auto direct prediction
    "-weightb", "1",                    # Weighted B-frames
    "-mixed-refs", "1",                 # Mixed references
    "-8x8dct", "1",                     # 8x8 DCT transform
    "-fast-pskip", "1",                 # Fast P-frame skip
    "-aq-mode", "2",                    # Adaptive quantization mode 2
    "-aq-strength", "1.0",              # AQ strength
    "-psy-rd", "1.0:0.15",              # Psychovisual rate-distortion
    "-b:v", "12M",                      # High bitrate for quality
    "-maxrate:v", "15M",                # Maximum bitrate
    "-bufsize:v", "24M",                # Large buffer for 128GB RAM
    "-c:a", "aac",                      # Audio codec
    "-b:a", "256k",                     # High quality audio
    "-movflags", "+faststart"           # Optimize for web playback
]

MIN_MATERIALS = 15               # S·ªë nguy√™n li·ªáu t·ªëi thi·ªÉu ƒë·ªÉ ch·ªçn (tƒÉng t·ª´ 12 l√™n 15)
MAX_MATERIALS = 30              # S·ªë nguy√™n li·ªáu t·ªëi ƒëa ƒë·ªÉ ch·ªçn
MAX_VIDEOS = 5                  # S·ªë video t·ªëi ƒëa
MIN_VIDEOS = 1                  # S·ªë video t·ªëi thi·ªÉu (n·∫øu c√≥)
VIDEO_CLIP_DURATION = 10        # Th·ªùi l∆∞·ª£ng m·ªói clip video (gi√¢y) - s·∫Ω ƒë∆∞·ª£c random 8-12s
VIDEO_CLIP_GAP = 1              # Kho·∫£ng tr·ªëng gi·ªØa c√°c clip video (gi√¢y)
MIN_VIDEO_CLIP_DURATION = 8     # Th·ªùi l∆∞·ª£ng clip video t·ªëi thi·ªÉu (gi√¢y) - tƒÉng t·ª´ 5s l√™n 8s
MAX_VIDEO_CLIP_DURATION = 12    # Th·ªùi l∆∞·ª£ng clip video t·ªëi ƒëa (gi√¢y)
MAX_IMAGE_DURATION = 5          # Th·ªùi l∆∞·ª£ng ·∫£nh t·ªëi ƒëa (gi√¢y)
AUDIO_FADEIN = 2.0              # Fade in cho nh·∫°c (gi√¢y)
AUDIO_FADEOUT = 5.0             # Fade out cho nh·∫°c (gi√¢y) - kh·ªõp v·ªõi outro
AUDIO_FADE_DURATION = 2.0       # Th·ªùi l∆∞·ª£ng fade in/out cho nh·∫°c (gi√¢y)
MAX_IMAGES_PER_FRAME = 5        # S·ªë ·∫£nh t·ªëi ƒëa trong 1 khung gh√©p

# ================= HI·ªÜU ·ª®NG CHUY·ªÇN C·∫¢NH =================
TRANSITION_PROBABILITY = 0.3    # 30% t·ªâ l·ªá c√≥ hi·ªáu ·ª©ng chuy·ªÉn c·∫£nh
TRANSITION_MIN_DURATION = 1.0   # Th·ªùi l∆∞·ª£ng t·ªëi thi·ªÉu hi·ªáu ·ª©ng chuy·ªÉn c·∫£nh (gi√¢y)
TRANSITION_MAX_DURATION = 2.5   # Th·ªùi l∆∞·ª£ng t·ªëi ƒëa hi·ªáu ·ª©ng chuy·ªÉn c·∫£nh (gi√¢y)
MAX_TOTAL_DURATION = 100         # Th·ªùi l∆∞·ª£ng t·ªëi ƒëa cho video (t√≠nh c·∫£ transitions)

# ================= OUTRO SETTINGS =================
OUTRO_DURATION = 5.0            # Th·ªùi l∆∞·ª£ng outro: fade out (gi√¢y)
# LOGO_TEXT = "EverLiving"        # Text logo (disabled - ImageMagick error)
# LOGO_COLOR = "blue"             # M√†u logo (disabled)
# LOGO_FONTSIZE = 80              # K√≠ch th∆∞·ªõc font (disabled)
# ===================================================

# Danh s√°ch 20 hi·ªáu ·ª©ng chuy·ªÉn c·∫£nh (kh√¥ng xoay v√≤ng)
TRANSITION_EFFECTS = [
    "fade",           # 1. M·ªù d·∫ßn
    "slide_left",     # 2. Tr∆∞·ª£t t·ª´ tr√°i
    "slide_right",    # 3. Tr∆∞·ª£t t·ª´ ph·∫£i  
    "slide_up",       # 4. Tr∆∞·ª£t t·ª´ tr√™n
    "slide_down",     # 5. Tr∆∞·ª£t t·ª´ d∆∞·ªõi
    "push_left",      # 6. ƒê·∫©y sang tr√°i
    "push_right",     # 7. ƒê·∫©y sang ph·∫£i
    "push_up",        # 8. ƒê·∫©y l√™n tr√™n
    "push_down",      # 9. ƒê·∫©y xu·ªëng d∆∞·ªõi
    "zoom_in",        # 10. Thu ph√≥ng v√†o
    "zoom_out",       # 11. Thu ph√≥ng ra
    "crossfade",      # 12. Ch·ªìng m·ªù

    # ===== 20 HI·ªÜU ·ª®NG M·ªöI (∆ØU TI√äN FADE) =====
    "fade_in_out",    # 21. Fade in + fade out k·∫øt h·ª£p
    "soft_fade",      # 22. Fade m·ªÅm m·∫°i h∆°n
    "double_fade",    # 23. Fade 2 l·ªõp
    "fade_zoom",      # 24. Fade + zoom nh·∫π
    "fade_slide",     # 25. Fade + slide nh·∫π
    "dissolve",       # 26. H√≤a tan
    "soft_dissolve",  # 27. H√≤a tan m·ªÅm
    "blur_fade",      # 28. Fade v·ªõi blur
    "gentle_slide",   # 29. Slide m·ªÅm m·∫°i
    "smooth_push",    # 30. Push m∆∞·ª£t m√†
    "soft_wipe",      # 31. Wipe m·ªÅm
    "fade_wipe",      # 32. Wipe + fade
    "elastic_fade",   # 33. Fade v·ªõi easing
    "bounce_fade",    # 34. Fade v·ªõi bounce nh·∫π
    "scale_fade",     # 35. Scale + fade
    "alpha_blend",    # 36. Alpha blending m∆∞·ª£t
    "gradient_fade",  # 37. Fade v·ªõi gradient
    "feather_fade",   # 38. Fade v·ªõi feather edge
    "soft_zoom",      # 39. Zoom m·ªÅm m·∫°i
    "gentle_morph"    # 40. Morph nh·∫π nh√†ng
]
# =====================================================

# ================= AI FACE DETECTION & SCORING =================
def check_file_readable(image_path):
    """
    Ki·ªÉm tra file c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c kh√¥ng
    """
    try:
        if not os.path.exists(image_path):
            return False, "File kh√¥ng t·ªìn t·∫°i"
        
        if not os.path.isfile(image_path):
            return False, "Kh√¥ng ph·∫£i file"
            
        # Th·ª≠ ƒë·ªçc v√†i bytes ƒë·∫ßu
        with open(image_path, 'rb') as f:
            header = f.read(10)
            if len(header) < 10:
                return False, "File r·ªóng ho·∫∑c b·ªã h·ªèng"
                
        return True, "OK"
    except Exception as e:
        return False, f"L·ªói ƒë·ªçc file: {str(e)}"

def detect_faces_and_score(image_path):
    """
    ü•á SUPER AI Face Detection: MediaPipe + Haar Cascade hybrid approach
    S·ª≠ d·ª•ng AI m·∫°nh nh·∫•t ƒë·ªÉ detect ch√≠nh x√°c khu√¥n m·∫∑t
    """
    try:
        # ƒê·ªçc ·∫£nh v·ªõi x·ª≠ l√Ω encoding UTF-8
        with open(image_path, 'rb') as f:
            file_bytes = np.frombuffer(f.read(), np.uint8)
            img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
            
        if img is None:
            return 0, 0, "Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh (format kh√¥ng h·ªó tr·ª£)"
        
        img_height, img_width = img.shape[:2]
        img_area = img_height * img_width
        
        # Resize if too large ƒë·ªÉ optimize processing
        original_scale = 1.0
        if max(img.shape[:2]) > 1200:
            scale = 1200 / max(img.shape[:2])
            new_width = int(img.shape[1] * scale)
            new_height = int(img.shape[0] * scale)
            img_resized = cv2.resize(img, (new_width, new_height))
            original_scale = 1 / scale
        else:
            img_resized = img.copy()
        
        all_detections = []
        
        # ========== METHOD 1: MediaPipe (Google's AI - Most Accurate) ==========
        try:
            import mediapipe as mp
            
            mp_face_detection = mp.solutions.face_detection
            
            # Test v·ªõi c·∫£ 2 models c·ªßa MediaPipe
            models = [
                (0, 0.6, 'mediapipe_short'),    # Short-range model
                (1, 0.6, 'mediapipe_full'),     # Full-range model
            ]
            
            for model_sel, min_conf, method_name in models:
                try:
                    with mp_face_detection.FaceDetection(
                        model_selection=model_sel, 
                        min_detection_confidence=min_conf
                    ) as face_detection:
                        # Convert BGR to RGB for MediaPipe
                        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
                        results_mp = face_detection.process(img_rgb)
                        
                        if results_mp.detections:
                            for detection in results_mp.detections:
                                # Get bounding box
                                bbox = detection.location_data.relative_bounding_box
                                h_res, w_res = img_resized.shape[:2]
                                
                                x = int(bbox.xmin * w_res * original_scale)
                                y = int(bbox.ymin * h_res * original_scale) 
                                w = int(bbox.width * w_res * original_scale)
                                h = int(bbox.height * h_res * original_scale)
                                
                                confidence = detection.score[0]
                                
                                # Basic validation
                                face_area = w * h
                                size_ratio = face_area / img_area
                                
                                if (0.001 <= size_ratio <= 0.5 and 
                                    w >= 20 and h >= 20 and
                                    x >= 0 and y >= 0 and
                                    x + w <= img_width and y + h <= img_height):
                                    
                                    detection_data = {
                                        'bbox': (x, y, w, h),
                                        'confidence': confidence,
                                        'method': method_name,
                                        'size_ratio': size_ratio,
                                        'ai_type': 'mediapipe'
                                    }
                                    
                                    all_detections.append(detection_data)
                except Exception:
                    continue
        except ImportError:
            pass
        except Exception:
            pass
        
        # ========== METHOD 2: Enhanced Haar Cascade (Backup) ==========
        try:
            gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)
            
            # CLAHE enhancement
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            enhanced = clahe.apply(gray)
            
            cascades = [
                ('haar_default', cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'),
                ('haar_alt', cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml'),
                ('haar_alt2', cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml'),
            ]
            
            for name, path in cascades:
                try:
                    cascade = cv2.CascadeClassifier(path)
                    if cascade.empty():
                        continue
                    
                    # Conservative parameters ƒë·ªÉ gi·∫£m false positive
                    for img_type, test_img in [('normal', gray), ('enhanced', enhanced)]:
                        faces = cascade.detectMultiScale(
                            test_img,
                            scaleFactor=1.08,
                            minNeighbors=4,
                            minSize=(25, 25),
                            maxSize=(test_img.shape[1]//2, test_img.shape[0]//2)
                        )
                        
                        for face in faces:
                            x, y, w, h = face
                            
                            # Scale back to original
                            x_orig = int(x * original_scale)
                            y_orig = int(y * original_scale)
                            w_orig = int(w * original_scale)
                            h_orig = int(h * original_scale)
                            
                            face_area = w_orig * h_orig
                            size_ratio = face_area / img_area
                            
                            if (0.001 <= size_ratio <= 0.4 and
                                w_orig >= 20 and h_orig >= 20 and
                                x_orig >= 0 and y_orig >= 0 and
                                x_orig + w_orig <= img_width and y_orig + h_orig <= img_height):
                                
                                confidence = 0.7 if name == 'haar_default' else 0.6
                                
                                detection_data = {
                                    'bbox': (x_orig, y_orig, w_orig, h_orig),
                                    'confidence': confidence,
                                    'method': f"{name}_{img_type}",
                                    'size_ratio': size_ratio,
                                    'ai_type': 'haar'
                                }
                                
                                all_detections.append(detection_data)
                except Exception:
                    continue
        except Exception:
            pass
        
        # ========== SMART VALIDATION & FILTERING ==========
        validated_faces = []
        
        for detection in all_detections:
            x, y, w, h = detection['bbox']
            
            # Extract face region for advanced validation
            if (y + h <= img_height and x + w <= img_width and y >= 0 and x >= 0):
                face_region = img[y:y+h, x:x+w]
                
                if face_region.size > 400:  # Only validate larger faces
                    face_gray = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)
                    
                    # 1. Texture Analysis
                    texture_var = np.var(face_gray)
                    
                    # 2. Edge Analysis
                    edges = cv2.Canny(face_gray, 30, 100)
                    edge_ratio = np.sum(edges > 0) / edges.size
                    
                    # 3. Gradient Analysis
                    sobelx = cv2.Sobel(face_gray, cv2.CV_64F, 1, 0, ksize=3)
                    sobely = cv2.Sobel(face_gray, cv2.CV_64F, 0, 1, ksize=3)
                    gradient_strength = np.mean(np.sqrt(sobelx**2 + sobely**2))
                    
                    # MediaPipe detections get more lenient validation (since they're more accurate)
                    if detection['ai_type'] == 'mediapipe':
                        # More lenient for MediaPipe
                        is_valid = (
                            texture_var > 50 and
                            edge_ratio > 0.02 and 
                            gradient_strength > 5
                        )
                    else:
                        # Stricter for Haar Cascade
                        is_valid = (
                            texture_var > 100 and
                            edge_ratio > 0.03 and
                            gradient_strength > 8
                        )
                    
                    if is_valid:
                        detection['validation_score'] = (texture_var/100 + edge_ratio*10 + gradient_strength/10) / 3
                        validated_faces.append(detection)
                else:
                    # Small faces - accept if from MediaPipe
                    if detection['ai_type'] == 'mediapipe':
                        validated_faces.append(detection)
        
        # ========== INTELLIGENT NMS (Non-Maximum Suppression) ==========
        if validated_faces:
            # Sort by AI type priority (MediaPipe > Haar) and confidence
            validated_faces.sort(
                key=lambda x: (1 if x['ai_type'] == 'mediapipe' else 0, x['confidence']), 
                reverse=True
            )
            
            final_faces = []
            for detection in validated_faces:
                x, y, w, h = detection['bbox']
                is_duplicate = False
                
                for existing in final_faces:
                    ex, ey, ew, eh = existing['bbox']
                    
                    # IoU calculation
                    overlap_x = max(0, min(x + w, ex + ew) - max(x, ex))
                    overlap_y = max(0, min(y + h, ey + eh) - max(y, ey))
                    overlap_area = overlap_x * overlap_y
                    
                    area1 = w * h
                    area2 = ew * eh
                    union_area = area1 + area2 - overlap_area
                    
                    if union_area > 0:
                        iou = overlap_area / union_area
                        
                        # Lower threshold for better faces
                        threshold = 0.2 if detection['ai_type'] == 'mediapipe' else 0.3
                        
                        if iou > threshold:
                            is_duplicate = True
                            # Keep MediaPipe detection over Haar if available
                            if (detection['ai_type'] == 'mediapipe' and 
                                existing['ai_type'] == 'haar'):
                                final_faces[final_faces.index(existing)] = detection
                            break
                
                if not is_duplicate:
                    final_faces.append(detection)
        else:
            final_faces = []
        
        # ========== SCORING ==========
        num_faces = len(final_faces)
        
        if num_faces == 0:
            return 0, 0, "‚ùå Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t (super-AI detection)"
        
        # Advanced scoring
        if num_faces == 1:
            face = final_faces[0]
            base_score = 30
            
            # AI type bonus
            ai_bonus = 8 if face['ai_type'] == 'mediapipe' else 5
            
            # Confidence bonus
            conf_bonus = min(10, face['confidence'] * 10)
            
            # Size bonus
            size_bonus = min(7, face['size_ratio'] * 100)
            
            face_score = base_score + ai_bonus + conf_bonus + size_bonus
        else:
            # Multiple faces
            base_score = 35
            
            # AI type bonus
            mediapipe_count = sum(1 for f in final_faces if f['ai_type'] == 'mediapipe')
            ai_bonus = mediapipe_count * 3
            
            # Average confidence bonus
            avg_confidence = np.mean([f['confidence'] for f in final_faces])
            conf_bonus = min(8, avg_confidence * 8)
            
            # Multi-face bonus
            multi_bonus = min(12, (num_faces - 1) * 4)
            
            face_score = base_score + ai_bonus + conf_bonus + multi_bonus
        
        face_score = min(40, max(0, int(face_score)))
        
        # Detailed report
        ai_types = [f['ai_type'] for f in final_faces]
        mediapipe_count = ai_types.count('mediapipe')
        haar_count = ai_types.count('haar')
        avg_conf = np.mean([f['confidence'] for f in final_faces])
        
        ai_info = []
        if mediapipe_count > 0:
            ai_info.append(f"MediaPipe:{mediapipe_count}")
        if haar_count > 0:
            ai_info.append(f"Haar:{haar_count}")
        
        detail = f"‚úÖ {num_faces} faces ({'+'.join(ai_info)}, conf:{avg_conf:.2f})"
        if num_faces > 1:
            detail += f" [+{min(12, (num_faces - 1) * 4)} multi-bonus]"
        
        return num_faces, face_score, detail
        
    except Exception as e:
        return 0, 0, f"L·ªói Super-AI detection: {str(e)[:50]}"
        
    except Exception as e:
        return 0, 0, f"L·ªói AI detection: {str(e)[:50]}"

def calculate_image_quality_score(image_path):
    """
    T√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng ·∫£nh (ƒë·ªô r√µ, ƒë·ªô s√°ng, m√†u s·∫Øc)
    
    Returns:
        tuple: (ƒëi·ªÉm_ch·∫•t_l∆∞·ª£ng, chi_ti·∫øt)
    """
    try:
        # ƒê·ªçc ·∫£nh v·ªõi x·ª≠ l√Ω encoding UTF-8
        with open(image_path, 'rb') as f:
            file_bytes = np.frombuffer(f.read(), np.uint8)
            img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
            
        if img is None:
            return 0, "Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh (format kh√¥ng h·ªó tr·ª£)"
        
        # 1. ƒê·ªô r√µ (Sharpness) - 15 ƒëi·ªÉm
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        sharpness_score = min(15, laplacian_var / 100)  # Normalize, max 15
        
        # 2. ƒê·ªô s√°ng (Brightness) - 15 ƒëi·ªÉm  
        brightness = np.mean(gray)
        # ƒêi·ªÉm cao nh·∫•t khi brightness trong kho·∫£ng 80-180
        if 80 <= brightness <= 180:
            brightness_score = 15
        else:
            brightness_score = max(0, 15 - abs(brightness - 130) / 10)
        
        # 3. C√¢n b·∫±ng m√†u s·∫Øc - 20 ƒëi·ªÉm
        # T√≠nh ƒë·ªô l·ªách chu·∫©n c·ªßa c√°c k√™nh m√†u
        b, g, r = cv2.split(img)
        color_balance = 20 - min(20, np.std([np.mean(b), np.mean(g), np.mean(r)]) / 5)
        
        total_quality = sharpness_score + brightness_score + color_balance
        detail = f"R√µ:{sharpness_score:.1f} S√°ng:{brightness_score:.1f} M√†u:{color_balance:.1f}"
        
        return total_quality, detail
        
    except Exception as e:
        return 0, f"L·ªói quality: {str(e)}"

def get_image_resolution_score(image_path):
    """
    Ki·ªÉm tra ƒë·ªô ph√¢n gi·∫£i ·∫£nh v√† t√≠nh ƒëi·ªÉm
    
    Returns:
        tuple: (ƒëi·ªÉm_ƒë·ªô_ph√¢n_gi·∫£i, width, height, is_acceptable, reason)
    """
    try:
        with Image.open(image_path) as img:
            width, height = img.size
        
        target_w, target_h = RESOLUTION  # 1280x720
        
        # Ki·ªÉm tra ·∫£nh c√≥ ƒë·∫°t ƒë·ªô ph√¢n gi·∫£i t·ªëi thi·ªÉu kh√¥ng (√≠t nh·∫•t 80% c·ªßa 720p)
        min_acceptable_w = target_w * 0.8  # 1024
        min_acceptable_h = target_h * 0.8  # 576
        is_acceptable = (width >= min_acceptable_w and height >= min_acceptable_h)
        
        # T·∫°o l√Ω do chi ti·∫øt
        if not is_acceptable:
            reason = f"ƒê·ªô ph√¢n gi·∫£i {width}x{height} < ti√™u chu·∫©n t·ªëi thi·ªÉu {min_acceptable_w:.0f}x{min_acceptable_h:.0f} (80% c·ªßa 720p)"
        else:
            reason = f"ƒê·ªô ph√¢n gi·∫£i {width}x{height} ƒë·∫°t chu·∫©n"
        
        # T√≠nh ƒëi·ªÉm ƒë·ªô ph√¢n gi·∫£i (0-10 ƒëi·ªÉm)
        if width >= target_w and height >= target_h:
            resolution_score = 10  # 720p ho·∫∑c cao h∆°n
        elif width >= target_w * 0.8 and height >= target_h * 0.8:
            resolution_score = 7   # 80% 720p
        elif width >= target_w * 0.6 and height >= target_h * 0.6:
            resolution_score = 4   # 60% 720p
        else:
            resolution_score = 1   # Qu√° nh·ªè
        
        return resolution_score, width, height, is_acceptable, reason
        
    except Exception as e:
        return 0, 0, 0, False, f"L·ªói ƒë·ªçc ·∫£nh: {str(e)}"

def calculate_total_image_score(image_path):
    """
    T√≠nh t·ªïng ƒëi·ªÉm ·∫£nh t·ª´ t·∫•t c·∫£ ti√™u ch√≠
    
    Returns:
        tuple: (t·ªïng_ƒëi·ªÉm, chi_ti·∫øt, is_acceptable, rejection_reason)
    """
    try:
        # Ki·ªÉm tra file c√≥ ƒë·ªçc ƒë∆∞·ª£c kh√¥ng
        can_read, read_error = check_file_readable(image_path)
        if not can_read:
            return 0, f"File error: {read_error}", False, f"File kh√¥ng ƒë·ªçc ƒë∆∞·ª£c: {read_error}"
        
        # 1. ƒêi·ªÉm khu√¥n m·∫∑t (0-40 ƒëi·ªÉm)
        num_faces, face_score, face_detail = detect_faces_and_score(image_path)
        
        # 2. ƒêi·ªÉm ch·∫•t l∆∞·ª£ng (0-50 ƒëi·ªÉm)
        quality_score, quality_detail = calculate_image_quality_score(image_path)
        
        # 3. ƒêi·ªÉm ƒë·ªô ph√¢n gi·∫£i (0-10 ƒëi·ªÉm)
        resolution_score, width, height, resolution_ok, resolution_reason = get_image_resolution_score(image_path)
        
        # T·ªïng ƒëi·ªÉm
        total_score = face_score + quality_score + resolution_score
        
        # Ki·ªÉm tra ti√™u ch√≠ lo·∫°i ·∫£nh v√† t·∫°o l√Ω do chi ti·∫øt
        rejection_reasons = []
        
        if total_score < 40:
            rejection_reasons.append(f"ƒêi·ªÉm th·∫•p {total_score:.1f}/40")
            
        if not resolution_ok:
            rejection_reasons.append(resolution_reason)
        
        is_acceptable = (total_score >= 40 and resolution_ok)
        rejection_reason = "; ".join(rejection_reasons) if rejection_reasons else "ƒê·∫°t ti√™u chu·∫©n"
        
        detail = f"Face:{face_score}({face_detail}) Quality:{quality_score:.1f}({quality_detail}) Res:{resolution_score}({width}x{height})"
        
        return total_score, detail, is_acceptable, rejection_reason
        
    except Exception as e:
        return 0, f"L·ªói t√≠nh ƒëi·ªÉm: {str(e)}", False, f"L·ªói x·ª≠ l√Ω: {str(e)}"

# =====================================================

def log(message):
    """Ghi log v·ªõi timestamp - Fixed Unicode for Windows"""
    import sys
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    try:
        print(f"[{timestamp}] {message}")
    except UnicodeEncodeError:
        # Fallback cho Windows PowerShell
        safe_message = message.encode('ascii', 'replace').decode('ascii')
        print(f"[{timestamp}] {safe_message}")

def generate_preview_thumbnail(video_path, output_dir):
    """
    Generate a preview thumbnail/GIF for uploaded videos
    Args:
        video_path: Path to the video file
        output_dir: Directory to save the preview
    Returns:
        Dict with success status and preview filename
    """
    try:
        if not os.path.exists(video_path):
            return {'success': False, 'message': 'Video file not found'}
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Generate preview filename
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        preview_filename = f"upload_preview_{video_name}.gif"
        preview_path = os.path.join(output_dir, preview_filename)
        
        # Load video with MoviePy
        video = VideoFileClip(video_path)
        
        # Get video duration and select clips
        duration = video.duration
        
        if duration <= 3:
            # Short video - use entire video
            preview_clip = video
        else:
            # Longer video - create a preview from first 3 seconds
            preview_clip = video.subclip(0, min(3, duration))
        
        # Resize to reasonable size for preview (max 320px width)
        if preview_clip.w > 320:
            preview_clip = preview_clip.resize(width=320)
        
        # Write as GIF with optimized settings
        preview_clip.write_gif(
            preview_path,
            fps=8,  # Lower FPS for smaller file size
            opt='OptimizePlus',  # Optimize for size
            fuzz=1  # Slight quality reduction for smaller size
        )
        
        # Clean up
        preview_clip.close()
        video.close()
        
        log(f"‚úÖ Generated video preview: {preview_filename}")
        
        return {
            'success': True,
            'preview_filename': preview_filename,
            'preview_path': preview_path
        }
        
    except Exception as e:
        log(f"‚ùå Error generating video preview: {str(e)}")
        return {
            'success': False,
            'message': f'Error generating preview: {str(e)}'
        }

def create_transition(clip1, clip2, transition_type, duration=1.0):
    """
    T·∫°o hi·ªáu ·ª©ng chuy·ªÉn c·∫£nh gi·ªØa 2 clips
    
    Args:
        clip1: Clip ƒë·∫ßu ti√™n
        clip2: Clip th·ª© hai  
        transition_type: Lo·∫°i hi·ªáu ·ª©ng (t·ª´ TRANSITION_EFFECTS)
        duration: Th·ªùi l∆∞·ª£ng hi·ªáu ·ª©ng (gi√¢y)
    
    Returns:
        Composite clip v·ªõi hi·ªáu ·ª©ng chuy·ªÉn c·∫£nh
    """
    w, h = RESOLUTION
    
    # ===== HI·ªÜU ·ª®NG G·ªêC =====
    if transition_type == "fade":
        clip1_fade = clip1.crossfadeout(duration)
        clip2_fade = clip2.crossfadein(duration)
        return concatenate_videoclips([clip1_fade, clip2_fade])
    
    elif transition_type == "slide_left":
        clip2_slide = clip2.set_position(lambda t: (-w + w*t/duration, 0) if t < duration else (0, 0))
        return CompositeVideoClip([clip1, clip2_slide], size=RESOLUTION)
        
    elif transition_type == "slide_right":
        clip2_slide = clip2.set_position(lambda t: (w - w*t/duration, 0) if t < duration else (0, 0))
        return CompositeVideoClip([clip1, clip2_slide], size=RESOLUTION)
        
    elif transition_type == "slide_up":
        clip2_slide = clip2.set_position(lambda t: (0, -h + h*t/duration) if t < duration else (0, 0))
        return CompositeVideoClip([clip1, clip2_slide], size=RESOLUTION)
        
    elif transition_type == "slide_down":
        clip2_slide = clip2.set_position(lambda t: (0, h - h*t/duration) if t < duration else (0, 0))
        return CompositeVideoClip([clip1, clip2_slide], size=RESOLUTION)
        
    elif transition_type == "zoom_in":
        clip2_zoom = clip2.resize(lambda t: 0.1 + 0.9*t/duration if t < duration else 1.0)
        clip2_zoom = clip2_zoom.set_position('center')
        return CompositeVideoClip([clip1, clip2_zoom], size=RESOLUTION)
        
    elif transition_type == "zoom_out":
        clip1_zoom = clip1.resize(lambda t: 1.0 + 2.0*t/duration if t < duration else 3.0)
        clip1_zoom = clip1_zoom.set_position('center')
        return CompositeVideoClip([clip1_zoom, clip2], size=RESOLUTION)
        
    elif transition_type == "crossfade":
        clip1_fade = clip1.fadeout(duration)
        clip2_fade = clip2.fadein(duration)
        return CompositeVideoClip([clip1_fade, clip2_fade], size=RESOLUTION)
    
    # ===== 20 HI·ªÜU ·ª®NG M·ªöI (∆ØU TI√äN FADE) =====
    elif transition_type == "fade_in_out":
        # Fade in + fade out k·∫øt h·ª£p v·ªõi th·ªùi gian d√†i h∆°n
        clip1_fade = clip1.fadeout(duration * 0.7)
        clip2_fade = clip2.fadein(duration * 0.7)
        return concatenate_videoclips([clip1_fade, clip2_fade])
        
    elif transition_type == "soft_fade":
        # Fade m·ªÅm m·∫°i h∆°n v·ªõi alpha th·∫•p
        clip1_soft = clip1.crossfadeout(duration * 1.2)
        clip2_soft = clip2.crossfadein(duration * 1.2)
        return concatenate_videoclips([clip1_soft, clip2_soft])
        
    elif transition_type == "double_fade":
        # Fade 2 l·ªõp v·ªõi opacity kh√°c nhau
        clip1_fade1 = clip1.fadeout(duration * 0.6)
        clip1_fade2 = clip1.fadeout(duration * 0.8)
        clip2_fade = clip2.fadein(duration)
        return CompositeVideoClip([clip1_fade1, clip1_fade2, clip2_fade], size=RESOLUTION)
        
    elif transition_type == "fade_zoom":
        # Fade + zoom nh·∫π
        clip1_fade = clip1.fadeout(duration).resize(lambda t: 1.0 + 0.1*t/duration)
        clip2_fade = clip2.fadein(duration).resize(lambda t: 0.9 + 0.1*t/duration)
        return concatenate_videoclips([clip1_fade, clip2_fade])
        
    elif transition_type == "fade_slide":
        # Fade + slide nh·∫π
        clip1_fade = clip1.fadeout(duration).set_position(lambda t: (-w*0.1*t/duration, 0))
        clip2_fade = clip2.fadein(duration).set_position(lambda t: (w*0.1*(1-t/duration), 0) if t < duration else (0, 0))
        return CompositeVideoClip([clip1_fade, clip2_fade], size=RESOLUTION)
        
    elif transition_type == "dissolve":
        # H√≤a tan v·ªõi opacity gradient
        clip1_dissolve = clip1.set_opacity(lambda t: 1.0 - t/duration if t < duration else 0)
        clip2_dissolve = clip2.set_opacity(lambda t: t/duration if t < duration else 1)
        return CompositeVideoClip([clip1_dissolve, clip2_dissolve], size=RESOLUTION)
        
    elif transition_type == "soft_dissolve":
        # H√≤a tan m·ªÅm v·ªõi th·ªùi gian d√†i h∆°n
        longer_duration = duration * 1.5
        clip1_soft = clip1.set_opacity(lambda t: 1.0 - t/longer_duration if t < longer_duration else 0)
        clip2_soft = clip2.set_opacity(lambda t: t/longer_duration if t < longer_duration else 1)
        return CompositeVideoClip([clip1_soft, clip2_soft], size=RESOLUTION)
        
    elif transition_type == "blur_fade":
        # Fade v·ªõi blur (gi·∫£ l·∫≠p b·∫±ng resize)
        clip1_blur = clip1.fadeout(duration).resize(lambda t: 1.0 - 0.1*t/duration)
        clip2_blur = clip2.fadein(duration).resize(lambda t: 0.9 + 0.1*t/duration)
        return concatenate_videoclips([clip1_blur, clip2_blur])
        
    elif transition_type == "gentle_slide":
        # Slide m·ªÅm m·∫°i v·ªõi easing
        clip2_gentle = clip2.set_position(lambda t: (-w * (1 - (t/duration)**2), 0) if t < duration else (0, 0))
        return CompositeVideoClip([clip1, clip2_gentle], size=RESOLUTION)
        
    elif transition_type == "smooth_push":
        # Push m∆∞·ª£t m√† v·ªõi easing
        clip1_push = clip1.set_position(lambda t: (-w * (t/duration)**2, 0) if t < duration else (-w, 0))
        clip2_push = clip2.set_position(lambda t: (w * (1 - (t/duration)**2), 0) if t < duration else (0, 0))
        return CompositeVideoClip([clip1_push, clip2_push], size=RESOLUTION)
    
    # ===== FALLBACK CHO C√ÅC HI·ªÜU ·ª®NG KH√ÅC =====
    else:
        # Fallback: fade ƒë∆°n gi·∫£n cho t·∫•t c·∫£ hi·ªáu ·ª©ng c√≤n l·∫°i
        clip1_fade = clip1.crossfadeout(duration)
        clip2_fade = clip2.crossfadein(duration)
        return concatenate_videoclips([clip1_fade, clip2_fade])

def apply_transitions_to_clips(clips):
    """
    √Åp d·ª•ng hi·ªáu ·ª©ng chuy·ªÉn c·∫£nh ng·∫´u nhi√™n cho danh s√°ch clips
    Logic m·ªõi: Duy·ªát t·ª´ng clip, random 30% cho m·ªói l·∫ßn chuy·ªÉn, ng∆∞ng khi g·∫ßn 100s
    
    Args:
        clips: Danh s√°ch c√°c clips
        
    Returns:
        Danh s√°ch clips ƒë√£ ƒë∆∞·ª£c √°p d·ª•ng transitions (t·ªëi ƒëa 100s)
    """
    video_timer.phase_update(f"√Åp d·ª•ng transitions cho {len(clips)} clips")
    if len(clips) <= 1:
        return clips

    log(f"B∆∞·ªõc 4.5: √Åp d·ª•ng hi·ªáu ·ª©ng chuy·ªÉn c·∫£nh")
    log(f"C√≥ {len(clips)} clips ‚Üí {len(clips)-1} l·∫ßn chuy·ªÉn c·∫£nh c√≥ th·ªÉ")
    log(f"T·ªâ l·ªá random: {TRANSITION_PROBABILITY*100:.0f}% m·ªói l·∫ßn chuy·ªÉn")
    log(f"Gi·ªõi h·∫°n t·ªëi ƒëa: {MAX_TOTAL_DURATION}s")
    
    # T√≠nh t·ªïng th·ªùi l∆∞·ª£ng hi·ªán t·∫°i (ch∆∞a c√≥ transitions)
    current_total_duration = sum(clip.duration for clip in clips)
    log(f"Th·ªùi l∆∞·ª£ng hi·ªán t·∫°i: {current_total_duration:.1f}s (ch∆∞a c√≥ hi·ªáu ·ª©ng)")
    
    final_clips = [clips[0]]  # Clip ƒë·∫ßu ti√™n lu√¥n ƒë∆∞·ª£c th√™m
    transitions_applied = 0
    total_transition_time = 0.0
    
    # Duy·ªát t·ª´ng clip v√† √°p d·ª•ng transition gi·ªØa c√°c clips
    for i in range(1, len(clips)):
        curr_clip = clips[i]  # ƒê·ªãnh nghƒ©a curr_clip ·ªü ƒë·∫ßu loop
        
        # Random 30% cho m·ªói l·∫ßn chuy·ªÉn t·ª´ clip (i-1) ‚Üí clip i
        should_add_transition = random.random() < TRANSITION_PROBABILITY
        
        if should_add_transition:
            # Random duration cho transition
            transition_duration = random.uniform(TRANSITION_MIN_DURATION, TRANSITION_MAX_DURATION)
            
            # Ki·ªÉm tra xem th√™m transition c√≥ v∆∞·ª£t qu√° 100s kh√¥ng
            estimated_total = current_total_duration + total_transition_time + transition_duration
            
            if estimated_total <= MAX_TOTAL_DURATION:
                # Ch·ªçn random hi·ªáu ·ª©ng
                transition_type = random.choice(TRANSITION_EFFECTS)
                transitions_applied += 1
                total_transition_time += transition_duration
                
                # Log chi ti·∫øt
                new_total = current_total_duration + total_transition_time
                log(f"Clip {i} ‚Üí {i+1}: ‚úÖ {transition_type} ({transition_duration:.1f}s) - T·ªïng: {new_total:.1f}s")
                
                try:
                    # T·∫°o transition gi·ªØa clip tr∆∞·ªõc v√† clip hi·ªán t·∫°i
                    prev_clip = clips[i-1]
                    curr_clip = clips[i]
                    
                    # T·∫°o overlap ƒë·ªÉ l√†m transition
                    overlap_duration = min(transition_duration, 
                                         prev_clip.duration * 0.3, 
                                         curr_clip.duration * 0.3)
                    
                    if overlap_duration > 0.1:  # Ch·ªâ l√†m transition n·∫øu ƒë·ªß th·ªùi gian
                        # T·∫°o transition v√† th√™m v√†o
                        prev_end = prev_clip.subclip(max(0, prev_clip.duration - overlap_duration))
                        curr_start = curr_clip.subclip(0, min(curr_clip.duration, overlap_duration))
                        
                        # T·∫°o transition
                        transition_clip = create_transition(prev_end, curr_start, transition_type, overlap_duration)
                        
                        # Gh√©p: [clip tr∆∞·ªõc - overlap] + [transition] + [clip sau - overlap]
                        if i == 1:  # Clip ƒë·∫ßu ti√™n c·∫ßn ƒë∆∞·ª£c c·∫Øt
                            prev_main = prev_clip.subclip(0, max(0, prev_clip.duration - overlap_duration))
                            final_clips[-1] = prev_main  # Thay th·∫ø clip ƒë·∫ßu
                            
                        final_clips.append(transition_clip)
                        
                        curr_main = curr_clip.subclip(min(curr_clip.duration, overlap_duration))
                        if curr_main.duration > 0.1:
                            final_clips.append(curr_main)
                        else:
                            final_clips.append(curr_clip)
                            
                    else:
                        # Kh√¥ng ƒë·ªß th·ªùi gian cho transition, th√™m clip b√¨nh th∆∞·ªùng
                        final_clips.append(curr_clip)
                        log(f"    ‚ö†Ô∏è Kh√¥ng ƒë·ªß th·ªùi gian cho transition, b·ªè qua")
                        
                except Exception as e:
                    # N·∫øu c√≥ l·ªói, th√™m clip b√¨nh th∆∞·ªùng
                    final_clips.append(curr_clip)
                    log(f"    ‚ùå L·ªói t·∫°o transition: {str(e)}, d√πng clip g·ªëc")
            else:
                # Th√™m transition s·∫Ω v∆∞·ª£t qu√° 100s
                final_clips.append(curr_clip)
                final_total = current_total_duration + total_transition_time
                log(f"Clip {i} ‚Üí {i+1}: ‚ö†Ô∏è D·ª´ng (g·∫ßn ch·∫°m 100s) - T·ªïng hi·ªán t·∫°i: {final_total:.1f}s")
                
                # Log c√°c clip c√≤n l·∫°i s·∫Ω kh√¥ng c√≥ transition
                for j in range(i+1, len(clips)):
                    log(f"Clip {j} ‚Üí {j+1}: ‚ùå Kh√¥ng c√≥ hi·ªáu ·ª©ng (gi·ªõi h·∫°n 100s)")
                break
        else:
            # Kh√¥ng c√≥ hi·ªáu ·ª©ng, th√™m clip b√¨nh th∆∞·ªùng
            final_clips.append(curr_clip)
            current_total = current_total_duration + total_transition_time
            log(f"Clip {i} ‚Üí {i+1}: ‚ùå Kh√¥ng c√≥ hi·ªáu ·ª©ng - T·ªïng: {current_total:.1f}s")
    
    # Th√™m c√°c clip c√≤n l·∫°i n·∫øu loop b·ªã break
    if len(final_clips) < len(clips):
        for remaining_i in range(len(final_clips), len(clips)):
            final_clips.append(clips[remaining_i])
    
    # Log k·∫øt qu·∫£ cu·ªëi c√πng
    final_total_duration = sum(clip.duration for clip in final_clips)
    log(f"‚Ä¢ K·∫æT QU·∫¢: {transitions_applied}/{len(clips)-1} transitions ƒë∆∞·ª£c √°p d·ª•ng")
    log(f"‚Ä¢ TH·ªúI L∆Ø·ª¢NG CU·ªêI: {final_total_duration:.1f}s")
    
    return final_clips

def add_outro_video(main_clip, outro_filename):
    """
    üÜï Th√™m outro video thay th·∫ø logo
    Args:
        main_clip: VideoClip ch√≠nh
        outro_filename: T√™n file outro video (t·ª´ orientation selection)
    Returns:
        VideoClip v·ªõi outro video ƒë∆∞·ª£c th√™m v√†o
    """
    from moviepy.editor import VideoFileClip, concatenate_videoclips
    
    try:
        outro_path = os.path.join(OUTRO_FOLDER, outro_filename)
        
        if not os.path.exists(outro_path):
            log(f"‚ö†Ô∏è File outro kh√¥ng t·ªìn t·∫°i: {outro_path}")
            return main_clip
            
        log(f"Th√™m outro video: {outro_filename}")
        
        # Load outro video
        outro_clip = VideoFileClip(outro_path)
        
        # üîß QUAN TR·ªåNG: Lo·∫°i b·ªè audio t·ª´ outro video ƒë·ªÉ tr√°nh conflict
        outro_clip = outro_clip.without_audio()
        
        # Resize outro video ƒë·ªÉ match resolution hi·ªán t·∫°i
        outro_clip = outro_clip.resize(RESOLUTION)
        
        # Gi·ªõi h·∫°n outro video t·ªëi ƒëa 5s
        if outro_clip.duration > 5:
            outro_clip = outro_clip.subclip(0, 5)
            log(f"   C·∫Øt outro video xu·ªëng 5s")
        
        log(f"   Outro duration: {outro_clip.duration:.1f}s (no audio)")
        
        # N·ªëi main clip + outro clip (outro kh√¥ng c√≥ audio)
        final_video = concatenate_videoclips([main_clip, outro_clip])
        
        log(f"‚úÖ Outro video ho√†n th√†nh: {final_video.duration:.1f}s")
        log(f"   {main_clip.duration:.1f}s main + {outro_clip.duration:.1f}s outro")
        
        # Clean up outro clip ƒë·ªÉ gi·∫£i ph√≥ng memory
        try:
            outro_clip.close()
            del outro_clip
        except:
            pass
        
        return final_video
        
    except Exception as e:
        log(f"‚ùå L·ªói th√™m outro video: {str(e)}")
        return main_clip


def add_outro_effect(main_clip):
    """
    Th√™m outro effect: Overlap transition trong 5s cu·ªëi
    - 0 ƒë·∫øn (duration-5)s: Video b√¨nh th∆∞·ªùng  
    - (duration-5)s ƒë·∫øn duration: Video fade out + Logo fade in ƒë·ªìng th·ªùi
    - T·ªïng duration: KH√îNG ƒê·ªîI
    
    Args:
        main_clip: Video clip ch√≠nh
        
    Returns:
        Video clip v·ªõi outro overlap transition (c√πng duration)
    """
    try:
        log(f"Th√™m outro overlap trong 5s cu·ªëi: video fade out + logo fade in")
        
        # 1. Chia video th√†nh 2 ph·∫ßn
        transition_start = max(0, main_clip.duration - OUTRO_DURATION)
        
        part1 = main_clip.subclip(0, transition_start)  # Ph·∫ßn b√¨nh th∆∞·ªùng
        part2 = main_clip.subclip(transition_start)     # 5s cu·ªëi ƒë·ªÉ fade out
        
        log(f"   Ph·∫ßn 1 (normal): 0-{transition_start:.1f}s")
        log(f"   Ph·∫ßn 2 (transition): {transition_start:.1f}-{main_clip.duration:.1f}s")
        
        # 2. T√¨m v√† load logo
        from moviepy.editor import ColorClip, CompositeVideoClip, ImageClip, concatenate_videoclips
        
        logo_folder = "./logo"
        logo_files = [f for f in os.listdir(logo_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        
        if logo_files:
            logo_path = os.path.join(logo_folder, logo_files[0])
            log(f"   Logo: {logo_files[0]} fade in trong 5s cu·ªëi")
            
            # 3. T·∫°o n·ªÅn tr·∫Øng fade in cho 5s cu·ªëi
            white_background = ColorClip(size=RESOLUTION, color=(255, 255, 255), duration=OUTRO_DURATION)
            white_background = white_background.fadein(OUTRO_DURATION)
            
            # 4. T·∫°o logo fade in cho 5s cu·ªëi
            logo_clip = ImageClip(logo_path, duration=OUTRO_DURATION, transparent=True)
            logo_width = RESOLUTION[0] // 3
            logo_clip = logo_clip.resize(width=logo_width).set_position('center')
            logo_clip = logo_clip.fadein(OUTRO_DURATION)
            
            # 5. Composite 5s cu·ªëi: Video fade out + N·ªÅn tr·∫Øng fade in + Logo fade in
            transition_part = CompositeVideoClip([
                part2.fadeout(OUTRO_DURATION),  # Video 5s cu·ªëi fade out
                white_background,               # N·ªÅn tr·∫Øng fade in
                logo_clip                      # Logo fade in
            ], size=RESOLUTION)
            
            # 6. N·ªëi: Ph·∫ßn b√¨nh th∆∞·ªùng + Ph·∫ßn transition
            final_clip = concatenate_videoclips([part1, transition_part])
            
            log(f"‚úÖ Outro overlap ho√†n th√†nh: {final_clip.duration:.1f}s")
            log(f"   {transition_start:.1f}s normal + {OUTRO_DURATION:.1f}s overlap = {final_clip.duration:.1f}s")
            
            return final_clip
        else:
            log("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y logo, ch·ªâ d√πng fade-out")
            # Fallback: fade out 5s cu·ªëi
            part1 = main_clip.subclip(0, transition_start)
            part2 = main_clip.subclip(transition_start).fadeout(OUTRO_DURATION)
            return concatenate_videoclips([part1, part2])
        
    except Exception as e:
        log(f"‚ùå L·ªói t·∫°o outro: {str(e)}, d√πng video g·ªëc")
        return main_clip

def list_music_files():
    """Li·ªát k√™ c√°c file nh·∫°c c√≥ s·∫µn"""
    if not os.path.exists(MUSIC_FOLDER):
        log(f"Th∆∞ m·ª•c nh·∫°c kh√¥ng t·ªìn t·∫°i: {MUSIC_FOLDER}")
        return []
    
    music_extensions = ['.mp3', '.wav', '.aac', '.m4a', '.flac']
    music_files = [f for f in os.listdir(MUSIC_FOLDER) 
                   if os.path.splitext(f)[1].lower() in music_extensions]
    
    log(f"T√¨m th·∫•y {len(music_files)} file nh·∫°c:")
    for i, music_file in enumerate(music_files, 1):
        log(f"  {i}. {music_file}")
    
    return music_files

def detect_main_subject(image_path_or_array):
    """
    AI detect ch·ªß th·ªÉ ch√≠nh trong ·∫£nh (faces, people, important objects)
    
    Returns:
        dict v·ªõi th√¥ng tin v√πng quan tr·ªçng: {
            'faces': [(x, y, w, h), ...],
            'main_region': (x, y, w, h),  # V√πng bao quanh t·∫•t c·∫£ subjects
            'crop_center': (cx, cy)       # ƒêi·ªÉm trung t√¢m t·ªëi ∆∞u ƒë·ªÉ crop
        }
    """
    try:
        # Load ·∫£nh
        if isinstance(image_path_or_array, str):
            img = cv2.imread(image_path_or_array)
        else:
            img = image_path_or_array
            
        if img is None:
            return None
            
        h, w = img.shape[:2]
        
        # 1. Face Detection v·ªõi OpenCV (fallback n·∫øu kh√¥ng c√≥ MediaPipe)
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        # 2. N·∫øu c√≥ MediaPipe, d√πng ƒë·ªÉ detect faces t·ªët h∆°n
        detected_faces = []
        try:
            import mediapipe as mp
            mp_face_detection = mp.solutions.face_detection
            
            with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:
                rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                results = face_detection.process(rgb_img)
                
                if results.detections:
                    for detection in results.detections:
                        bbox = detection.location_data.relative_bounding_box
                        x = int(bbox.xmin * w)
                        y = int(bbox.ymin * h)
                        face_w = int(bbox.width * w)
                        face_h = int(bbox.height * h)
                        detected_faces.append((x, y, face_w, face_h))
        except:
            # Fallback to OpenCV faces n·∫øu MediaPipe kh√¥ng c√≥
            detected_faces = [(x, y, w, h) for (x, y, w, h) in faces]
        
        # 3. T√≠nh v√πng quan tr·ªçng
        if detected_faces:
            # C√≥ faces - t√≠nh bounding box bao quanh t·∫•t c·∫£ faces
            min_x = min([x for x, y, w, h in detected_faces])
            min_y = min([y for x, y, w, h in detected_faces])
            max_x = max([x + w for x, y, w, h in detected_faces])
            max_y = max([y + h for x, y, w, h in detected_faces])
            
            # M·ªü r·ªông v√πng m·ªôt ch√∫t ƒë·ªÉ kh√¥ng crop qu√° s√°t
            margin_x = int((max_x - min_x) * 0.3)
            margin_y = int((max_y - min_y) * 0.3)
            
            main_x = max(0, min_x - margin_x)
            main_y = max(0, min_y - margin_y)
            main_w = min(w - main_x, max_x - min_x + 2 * margin_x)
            main_h = min(h - main_y, max_y - min_y + 2 * margin_y)
            
            crop_center_x = main_x + main_w // 2
            crop_center_y = main_y + main_h // 2
            
        else:
            # Kh√¥ng c√≥ faces - d√πng center crop
            main_x, main_y = 0, 0
            main_w, main_h = w, h
            crop_center_x, crop_center_y = w // 2, h // 2
        
        return {
            'faces': detected_faces,
            'main_region': (main_x, main_y, main_w, main_h),
            'crop_center': (crop_center_x, crop_center_y),
            'image_size': (w, h)
        }
        
    except Exception as e:
        # Fallback: center crop
        return {
            'faces': [],
            'main_region': (0, 0, w if 'w' in locals() else 100, h if 'h' in locals() else 100),
            'crop_center': (w//2 if 'w' in locals() else 50, h//2 if 'h' in locals() else 50),
            'image_size': (w if 'w' in locals() else 100, h if 'h' in locals() else 100)
        }

def analyze_image_for_collage(image_path):
    """
    Ph√¢n t√≠ch ·∫£nh ƒë·ªÉ ƒë∆∞a ra crop strategy ph√π h·ª£p cho collage
    
    Returns:
        dict: {
            'aspect_ratio': float,
            'is_portrait': bool, 
            'is_landscape': bool,
            'recommended_position': str,  # 'main', 'side', 'any'
            'crop_strategy': str,         # 'center', 'left_bias', 'right_bias'
            'priority_score': float       # ƒêi·ªÉm ∆∞u ti√™n cho main position
        }
    """
    try:
        # Get image dimensions
        img = cv2.imread(image_path)
        if img is None:
            return None
            
        h, w = img.shape[:2]
        aspect_ratio = w / h
        
        # Analyze image characteristics
        is_portrait = aspect_ratio < 0.8   # T·ª∑ l·ªá < 4:5
        is_landscape = aspect_ratio > 1.25  # T·ª∑ l·ªá > 5:4
        is_square = 0.8 <= aspect_ratio <= 1.25
        
        # Calculate priority score for main position
        priority_score = 0.5  # Base score
        
        if is_portrait:
            priority_score += 0.3  # Portrait ∆∞u ti√™n main position
        elif is_square:
            priority_score += 0.1  # Square c≈©ng ok cho main
        
        # Face detection bonus (n·∫øu c√≥ face th√¨ ∆∞u ti√™n main)
        try:
            rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            mp_face_detection = mp.solutions.face_detection
            
            with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.3) as face_detection:
                results = face_detection.process(rgb_img)
                if results.detections and len(results.detections) > 0:
                    priority_score += 0.2  # C√≥ face = ∆∞u ti√™n main
                    
        except Exception:
            pass  # Ignore face detection errors
        
        # Determine recommended position
        if priority_score > 0.7:
            recommended_position = 'main'
        elif priority_score < 0.4:
            recommended_position = 'side'
        else:
            recommended_position = 'any'
        
        # Determine crop strategy
        if is_landscape:
            # Landscape c√≥ th·ªÉ c√≥ nhi·ªÅu subjects ‚Üí c·∫ßn bias strategy
            crop_strategy = 'smart_bias'  # S·∫Ω test c·∫£ left v√† right bias
        else:
            # Portrait/square th∆∞·ªùng d√πng center crop
            crop_strategy = 'center'
        
        return {
            'aspect_ratio': aspect_ratio,
            'is_portrait': is_portrait,
            'is_landscape': is_landscape,
            'is_square': is_square,
            'recommended_position': recommended_position,
            'crop_strategy': crop_strategy,
            'priority_score': priority_score,
            'dimensions': (w, h)
        }
        
    except Exception as e:
        log(f"    ‚ö†Ô∏è L·ªói analyze image {image_path}: {str(e)}")
        return None

def smart_resize_image_enhanced(img_clip, target_resolution=(1920, 1080), fill_mode='smart_crop', position_hint=None):
    """
    Enhanced smart resize v·ªõi position-aware cropping
    
    Args:
        img_clip: ImageClip object
        target_resolution: tuple (width, height)
        fill_mode: 'smart_crop', 'center', 'left_bias', 'right_bias'
        position_hint: 'main', 'side' - hint v·ªÅ v·ªã tr√≠ trong collage
    """
    target_w, target_h = target_resolution
    img_w, img_h = img_clip.w, img_clip.h
    
    if fill_mode == 'smart_crop':
        # Enhanced smart cropping v·ªõi position awareness
        try:
            temp_array = img_clip.get_frame(0)
            bgr_array = cv2.cvtColor(temp_array, cv2.COLOR_RGB2BGR)
            
            # Detect subjects
            subject_info = detect_main_subject(bgr_array)
            
            if subject_info and subject_info['crop_center']:
                cx, cy = subject_info['crop_center']
                
                # Apply position-aware bias cho landscape images
                if img_w > img_h * 1.3:  # Landscape image
                    if position_hint == 'side' and len(subject_info['faces']) > 1:
                        # Side position v·ªõi nhi·ªÅu faces ‚Üí bias ƒë·ªÉ tr√°nh c·∫Øt qua subjects
                        faces = subject_info['faces']
                        if len(faces) >= 2:
                            # Ch·ªçn face ph√π h·ª£p v·ªõi target aspect ratio
                            target_aspect = target_w / target_h
                            
                            if target_aspect < 0.8:  # Target is tall (side position)
                                # Ch·ªçn face ·ªü edge ƒë·ªÉ tr√°nh c·∫Øt qua gi·ªØa
                                leftmost = min(faces, key=lambda f: f[0] + f[2]/2)
                                rightmost = max(faces, key=lambda f: f[0] + f[2]/2)
                                
                                left_center = leftmost[0] + leftmost[2]/2
                                right_center = rightmost[0] + rightmost[2]/2
                                
                                # Ch·ªçn bias strategy d·ª±a tr√™n target position
                                if right_center - left_center > img_w * 0.3:  # Faces c√°ch xa nhau
                                    # Bias v·ªÅ left n·∫øu c√≥ space
                                    if left_center > img_w * 0.3:
                                        cx = left_center
                                        log(f"    üéØ Applied LEFT bias for side position")
                                    else:
                                        cx = right_center  
                                        log(f"    üéØ Applied RIGHT bias for side position")
                
                crop_coords = smart_crop_calculation(img_w, img_h, target_w, target_h, (cx, cy))
                x1, y1, x2, y2 = crop_coords
                
                scale_w = target_w / img_w
                scale_h = target_h / img_h
                scale = max(scale_w, scale_h)
                
                resized_img = img_clip.resize(scale)
                final_clip = resized_img.crop(x1=x1*scale, y1=y1*scale, x2=x2*scale, y2=y2*scale)
                
                return final_clip
                
            else:
                # Fallback to smart bias for landscape without detected subjects
                if img_w > img_h * 1.3:  # Landscape
                    if position_hint == 'side':
                        # Side position ‚Üí left bias th∆∞·ªùng t·ªët h∆°n
                        cx = img_w // 3
                        log(f"    üéØ Applied LEFT bias fallback for landscape in side position")
                    else:
                        cx = img_w // 2  # Center for main position
                else:
                    cx = img_w // 2
                    
                cy = img_h // 2
                crop_coords = smart_crop_calculation(img_w, img_h, target_w, target_h, (cx, cy))
                x1, y1, x2, y2 = crop_coords
                
                scale_w = target_w / img_w
                scale_h = target_h / img_h
                scale = max(scale_w, scale_h)
                
                resized_img = img_clip.resize(scale)
                final_clip = resized_img.crop(x1=x1*scale, y1=y1*scale, x2=x2*scale, y2=y2*scale)
                
                return final_clip
                
        except Exception as e:
            log(f"    ‚ùå Enhanced smart crop error: {str(e)}, fallback to center")
            
    # Fallback to original smart_resize_image
    return smart_resize_image(img_clip, target_resolution, 'smart_crop')

def smart_crop_calculation(source_w, source_h, target_w, target_h, crop_center=None):
    """
    T√≠nh to√°n v√πng crop th√¥ng minh
    
    Args:
        source_w, source_h: K√≠ch th∆∞·ªõc ·∫£nh g·ªëc
        target_w, target_h: K√≠ch th∆∞·ªõc ƒë√≠ch
        crop_center: (cx, cy) ƒëi·ªÉm trung t√¢m ∆∞u ti√™n, None = center crop
    
    Returns:
        (x1, y1, x2, y2): T·ªça ƒë·ªô v√πng crop
    """
    if crop_center is None:
        crop_center = (source_w // 2, source_h // 2)
    
    cx, cy = crop_center
    
    # T√≠nh t·ª∑ l·ªá scale c·∫ßn thi·∫øt
    scale_w = target_w / source_w
    scale_h = target_h / source_h
    scale = max(scale_w, scale_h)  # Scale ƒë·ªÉ fill ƒë·∫ßy target
    
    # K√≠ch th∆∞·ªõc sau khi scale
    scaled_w = int(source_w * scale)
    scaled_h = int(source_h * scale)
    
    # T√≠nh v√πng crop t·ª´ ·∫£nh ƒë√£ scale
    crop_w = target_w
    crop_h = target_h
    
    # ƒêi·ªÅu ch·ªânh center ƒë·ªÉ crop kh√¥ng b·ªã tr√†n
    crop_x = max(0, min(scaled_w - crop_w, int(cx * scale) - crop_w // 2))
    crop_y = max(0, min(scaled_h - crop_h, int(cy * scale) - crop_h // 2))
    
    # Convert v·ªÅ t·ªça ƒë·ªô ·∫£nh g·ªëc
    x1 = crop_x / scale
    y1 = crop_y / scale
    x2 = x1 + crop_w / scale
    y2 = y1 + crop_h / scale
    
    return (x1, y1, x2, y2)

def smart_resize_image(img_clip, target_resolution=(1920, 1080), fill_mode='smart_crop'):
    """
    Resize ·∫£nh th√¥ng minh v·ªõi AI-powered cropping
    
    Args:
        img_clip: ImageClip object
        target_resolution: tuple (width, height) - resolution ƒë√≠ch
        fill_mode: 'letterbox', 'crop', 'stretch', 'smart_crop' (AI-powered)
    
    Returns:
        ImageClip ƒë√£ ƒë∆∞·ª£c resize
    """
    target_w, target_h = target_resolution
    img_w, img_h = img_clip.w, img_clip.h
    
    if fill_mode == 'smart_crop':
        # AI Smart Cropping
        try:
            # L·∫•y frame ƒë·∫ßu ti√™n c·ªßa ImageClip ƒë·ªÉ analyze
            temp_array = img_clip.get_frame(0)  # RGB array
            
            # Convert RGB to BGR cho OpenCV
            bgr_array = cv2.cvtColor(temp_array, cv2.COLOR_RGB2BGR)
            
            # Detect ch·ªß th·ªÉ ch√≠nh
            subject_info = detect_main_subject(bgr_array)
            
            if subject_info and subject_info['crop_center']:
                # C√≥ detect ƒë∆∞·ª£c subject - crop th√¥ng minh
                cx, cy = subject_info['crop_center']
                crop_coords = smart_crop_calculation(img_w, img_h, target_w, target_h, (cx, cy))
                x1, y1, x2, y2 = crop_coords
                
                # Scale ƒë·ªÉ fill ƒë·∫ßy target resolution
                scale_w = target_w / img_w
                scale_h = target_h / img_h
                scale = max(scale_w, scale_h)
                
                # Resize tr∆∞·ªõc
                resized_img = img_clip.resize(scale)
                
                # Sau ƒë√≥ crop
                final_clip = resized_img.crop(
                    x1=x1*scale, y1=y1*scale, 
                    x2=x2*scale, y2=y2*scale
                )
                
                log(f"    üéØ Smart crop: detected {len(subject_info['faces'])} faces, center=({cx},{cy})")
                return final_clip
                
            else:
                # Fallback to center crop
                log(f"    ‚ö†Ô∏è Smart crop fallback: no subjects detected, using center crop")
                return smart_resize_image(img_clip, target_resolution, 'crop')
                
        except Exception as e:
            log(f"    ‚ùå Smart crop error: {str(e)}, fallback to center crop")
            return smart_resize_image(img_clip, target_resolution, 'crop')
    
    elif fill_mode == 'letterbox':
        # Gi·ªØ t·ª∑ l·ªá g·ªëc, th√™m padding (n·ªÅn ƒëen) n·∫øu c·∫ßn
        # Scale ƒë·ªÉ ·∫£nh v·ª´a v·ªõi m·ªôt chi·ªÅu c·ªßa khung h√¨nh
        scale_w = target_w / img_w
        scale_h = target_h / img_h
        scale = min(scale_w, scale_h)  # Ch·ªçn scale nh·ªè h∆°n ƒë·ªÉ kh√¥ng b·ªã tr√†n
        
        # Resize ·∫£nh
        new_w = int(img_w * scale)
        new_h = int(img_h * scale)
        resized_img = img_clip.resize((new_w, new_h))
        
        # T·∫°o n·ªÅn ƒëen v√† ƒë·∫∑t ·∫£nh v√†o gi·ªØa
        black_bg = ColorClip(size=target_resolution, color=(0, 0, 0))
        final_clip = CompositeVideoClip([
            black_bg,
            resized_img.set_position('center')
        ], size=target_resolution)
        
        return final_clip
        
    elif fill_mode == 'crop':
        # C·∫Øt ·∫£nh ƒë·ªÉ v·ª´a khung h√¨nh (c√≥ th·ªÉ m·∫•t m·ªôt ph·∫ßn ·∫£nh)
        scale_w = target_w / img_w
        scale_h = target_h / img_h
        scale = max(scale_w, scale_h)  # Ch·ªçn scale l·ªõn h∆°n ƒë·ªÉ fill ƒë·∫ßy
        
        # Resize v√† crop
        resized_img = img_clip.resize(scale)
        
        # Crop v·ªÅ ƒë√∫ng k√≠ch th∆∞·ªõc
        return resized_img.crop(
            x_center=resized_img.w/2, 
            y_center=resized_img.h/2,
            width=target_w, 
            height=target_h
        )
        
    else:  # stretch
        # K√©o d√£n ·∫£nh ƒë·ªÉ v·ª´a khung h√¨nh (c√≥ th·ªÉ b·ªã bi·∫øn d·∫°ng)
        return img_clip.resize(target_resolution)

def smart_resize_video(video_clip, target_resolution=(1920, 1080), target_fps=24, fill_mode='smart_crop'):
    """
    Resize video th√¥ng minh v·ªõi AI-powered cropping
    
    Args:
        video_clip: VideoFileClip object
        target_resolution: tuple (width, height) - resolution ƒë√≠ch
        target_fps: int - FPS ƒë√≠ch cho smooth playback
        fill_mode: 'letterbox', 'crop', 'smart_crop' (AI-powered)
    
    Returns:
        VideoFileClip ƒë√£ ƒë∆∞·ª£c resize m∆∞·ª£t m√†
    """
    target_w, target_h = target_resolution
    video_w, video_h = video_clip.w, video_clip.h
    
    # ƒê·∫∑t FPS tr∆∞·ªõc ƒë·ªÉ tr√°nh gi·∫≠t
    if hasattr(video_clip, 'fps') and video_clip.fps:
        if video_clip.fps != target_fps:
            video_clip = video_clip.set_fps(target_fps)
    
    if fill_mode == 'smart_crop':
        # AI Smart Cropping cho video
        try:
            # Sample m·ªôt v√†i frame ƒë·ªÉ analyze
            sample_times = [0.1, 0.3, 0.5, 0.7, 0.9]  # 5 ƒëi·ªÉm th·ªùi gian
            sample_times = [t * video_clip.duration for t in sample_times if t * video_clip.duration < video_clip.duration]
            
            all_centers = []
            total_faces = 0
            
            for t in sample_times:
                try:
                    frame = video_clip.get_frame(t)  # RGB array
                    bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    
                    subject_info = detect_main_subject(bgr_frame)
                    if subject_info and subject_info['crop_center']:
                        all_centers.append(subject_info['crop_center'])
                        total_faces += len(subject_info['faces'])
                        
                except Exception as e:
                    continue
            
            if all_centers:
                # T√≠nh trung b√¨nh weighted c·ªßa t·∫•t c·∫£ centers
                avg_cx = sum([cx for cx, cy in all_centers]) // len(all_centers)
                avg_cy = sum([cy for cx, cy in all_centers]) // len(all_centers)
                
                # Smart crop v·ªõi center ƒë√£ t√≠nh to√°n
                scale_w = target_w / video_w
                scale_h = target_h / video_h
                scale = max(scale_w, scale_h)  # Scale ƒë·ªÉ fill ƒë·∫ßy
                
                # Resize tr∆∞·ªõc
                resized_video = video_clip.resize(scale)
                
                # T√≠nh to√°n crop region
                crop_coords = smart_crop_calculation(video_w, video_h, target_w, target_h, (avg_cx, avg_cy))
                x1, y1, x2, y2 = crop_coords
                
                # Crop video
                final_video = resized_video.crop(
                    x1=x1*scale, y1=y1*scale,
                    x2=x2*scale, y2=y2*scale
                )
                
                log(f"      üéØ Smart crop video: {total_faces} faces detected, center=({avg_cx},{avg_cy})")
                return final_video
                
            else:
                # Fallback to center crop
                log(f"      ‚ö†Ô∏è Smart crop video fallback: no subjects detected, using center crop")
                return smart_resize_video(video_clip, target_resolution, target_fps, 'crop')
                
        except Exception as e:
            log(f"      ‚ùå Smart crop video error: {str(e)}, fallback to center crop")
            return smart_resize_video(video_clip, target_resolution, target_fps, 'crop')
    
    elif fill_mode == 'letterbox':
        # Letterbox mode - gi·ªØ t·ª∑ l·ªá, th√™m padding
        scale_w = target_w / video_w
        scale_h = target_h / video_h
        scale = min(scale_w, scale_h)  # Letterbox ƒë·ªÉ kh√¥ng b·ªã bi·∫øn d·∫°ng
        
        # Resize video v·ªõi scaling m∆∞·ª£t
        new_w = int(video_w * scale)
        new_h = int(video_h * scale)
        
        resized_video = video_clip.resize((new_w, new_h))
        
        # T·∫°o n·ªÅn ƒëen v√† ƒë·∫∑t video v√†o gi·ªØa
        black_bg = ColorClip(size=target_resolution, color=(0, 0, 0))
        final_video = CompositeVideoClip([
            black_bg.set_duration(video_clip.duration),
            resized_video.set_position('center')
        ], size=target_resolution)
        
        return final_video
        
    elif fill_mode == 'crop':
        # Center crop mode
        scale_w = target_w / video_w
        scale_h = target_h / video_h
        scale = max(scale_w, scale_h)  # Scale ƒë·ªÉ fill ƒë·∫ßy
        
        # Resize v√† crop
        resized_video = video_clip.resize(scale)
        
        return resized_video.crop(
            x_center=resized_video.w/2,
            y_center=resized_video.h/2,
            width=target_w,
            height=target_h
        )
        
    else:  # stretch
        return video_clip.resize(target_resolution)
    resized_video = video_clip.resize((new_w, new_h))
    
    # N·∫øu video kh√¥ng fill ƒë·∫ßy, th√™m padding ƒëen
    if new_w != target_w or new_h != target_h:
        # T·∫°o background ƒëen
        black_bg = ColorClip(size=target_resolution, color=(0, 0, 0))
        
        # Composite video l√™n background
        final_video = CompositeVideoClip([
            black_bg.set_duration(resized_video.duration),
            resized_video.set_position('center')
        ], size=target_resolution)
        
        return final_video
    else:
        return resized_video

def create_image_collage(image_paths, duration=4.0):
    """T·∫°o collage t·ª´ nhi·ªÅu ·∫£nh v·ªõi intelligent positioning v√† smart crop"""
    try:
        num_images = len(image_paths)
        log(f"    T·∫°o collage t·ª´ {num_images} ·∫£nh v·ªõi intelligent positioning")
        
        # Professional margin v√† spacing
        margin = max(10, min(RESOLUTION) // 100)  # Dynamic margin based on resolution
        spacing = margin // 2  # Spacing gi·ªØa c√°c ·∫£nh
        
        if num_images == 1:
            # ·∫¢nh ƒë∆°n - s·ª≠ d·ª•ng smart resize ƒë·ªÉ v·ª´a khung h√¨nh
            img = ImageClip(image_paths[0])
            collage = smart_resize_image_enhanced(img, RESOLUTION, 'smart_crop')
            log(f"    ‚úÖ ·∫¢nh ƒë∆°n v·ªõi enhanced smart crop")
            
        else:
            # Ph√¢n t√≠ch t·ª´ng ·∫£nh ƒë·ªÉ intelligent positioning
            image_analyses = []
            for i, img_path in enumerate(image_paths):
                analysis = analyze_image_for_collage(img_path)
                if analysis:
                    analysis['original_index'] = i
                    analysis['path'] = img_path
                    image_analyses.append(analysis)
                    log(f"    üìä ·∫¢nh {i+1}: {analysis['dimensions'][0]}x{analysis['dimensions'][1]}, "
                        f"priority={analysis['priority_score']:.2f}, pos={analysis['recommended_position']}")
            
            # Sort ·∫£nh theo priority score (cao nh·∫•t l√™n ƒë·∫ßu cho main position)
            image_analyses.sort(key=lambda x: x['priority_score'], reverse=True)
            sorted_paths = [analysis['path'] for analysis in image_analyses]
            
            # Load images theo th·ª© t·ª± ƒë√£ sort
            images = []
            for img_path in sorted_paths:
                img = ImageClip(img_path)
                images.append(img)
            
            # Available space sau khi tr·ª´ margin
            available_w = RESOLUTION[0] - (2 * margin)
            available_h = RESOLUTION[1] - (2 * margin)
            
            if num_images == 2:
                # 2 ·∫£nh - modern split layout v·ªõi spacing
                cell_w = (available_w - spacing) // 2
                cell_h = available_h
                
                # Enhanced crop v·ªõi position hints
                img1 = smart_resize_image_enhanced(images[0], (cell_w, cell_h), 'smart_crop', 'main').set_position((margin, margin))
                img2 = smart_resize_image_enhanced(images[1], (cell_w, cell_h), 'smart_crop', 'main').set_position((margin + cell_w + spacing, margin))
                
                collage = CompositeVideoClip([img1, img2], size=RESOLUTION)
                log(f"    ‚úÖ Layout 2 ·∫£nh v·ªõi intelligent positioning")
                
            elif num_images == 3:
                # 3 ·∫£nh - Featured + sidebar layout v·ªõi intelligent positioning
                main_w = int(available_w * 0.65)  # 65% cho ·∫£nh ch√≠nh (t∆∞∆°ng t·ª± test)
                side_w = available_w - main_w - spacing
                side_h = (available_h - spacing) // 2
                
                # ·∫¢nh c√≥ priority cao nh·∫•t ‚Üí main position (tr√°i)
                # 2 ·∫£nh c√≤n l·∫°i ‚Üí side positions (ph·∫£i)
                img1 = smart_resize_image_enhanced(images[0], (main_w, available_h), 'smart_crop', 'main').set_position((margin, margin))
                img2 = smart_resize_image_enhanced(images[1], (side_w, side_h), 'smart_crop', 'side').set_position((margin + main_w + spacing, margin))
                img3 = smart_resize_image_enhanced(images[2], (side_w, side_h), 'smart_crop', 'side').set_position((margin + main_w + spacing, margin + side_h + spacing))
                
                collage = CompositeVideoClip([img1, img2, img3], size=RESOLUTION)
                log(f"    ‚úÖ Layout 3 ·∫£nh: Main={image_analyses[0]['priority_score']:.2f}, "
                    f"Side1={image_analyses[1]['priority_score']:.2f}, Side2={image_analyses[2]['priority_score']:.2f}")
                
            elif num_images == 4:
                # 4 ·∫£nh - Perfect grid 2x2 v·ªõi intelligent positioning
                cell_w = (available_w - spacing) // 2
                cell_h = (available_h - spacing) // 2
                
                positions = [
                    (margin, margin),  # Top-left (highest priority)
                    (margin + cell_w + spacing, margin),  # Top-right
                    (margin, margin + cell_h + spacing),  # Bottom-left
                    (margin + cell_w + spacing, margin + cell_h + spacing)  # Bottom-right
                ]
                
                clips = []
                for i, img in enumerate(images[:4]):
                    # First position gets priority hint as main
                    position_hint = 'main' if i == 0 else 'any'
                    clip = smart_resize_image_enhanced(img, (cell_w, cell_h), 'smart_crop', position_hint).set_position(positions[i])
                    clips.append(clip)
                
                collage = CompositeVideoClip(clips, size=RESOLUTION)
                log(f"    ‚úÖ Grid 2x2 v·ªõi ·∫£nh priority cao nh·∫•t ·ªü top-left")
                
            else:  # 4 ·∫£nh - Grid 2x2 layout
                # Gi·ªõi h·∫°n t·ªëi ƒëa 4 ·∫£nh ƒë·ªÉ tr√°nh ph·ª©c t·∫°p
                images = images[:4]  # Ch·ªâ l·∫•y 4 ·∫£nh t·ªët nh·∫•t (ƒë√£ sorted theo priority)
                
                # Layout 2x2 grid
                cell_w = (available_w - spacing) // 2
                cell_h = (available_h - spacing) // 2
                
                positions = [
                    (margin, margin),  # Top-left
                    (margin + cell_w + spacing, margin),  # Top-right
                    (margin, margin + cell_h + spacing),  # Bottom-left
                    (margin + cell_w + spacing, margin + cell_h + spacing)  # Bottom-right
                ]
                
                clips = []
                for i in range(min(4, len(images))):
                    # ·∫¢nh ƒë·∫ßu ti√™n (priority cao nh·∫•t) d√πng main hint, c√≤n l·∫°i d√πng side hint
                    hint = 'main' if i == 0 else 'side'
                    clip = smart_resize_image_enhanced(images[i], (cell_w, cell_h), 'smart_crop', hint).set_position(positions[i])
                    clips.append(clip)
                
                collage = CompositeVideoClip(clips, size=RESOLUTION)
                log(f"    ‚úÖ Grid 2x2 v·ªõi ·∫£nh priority cao nh·∫•t ·ªü top-left (t·ªëi ƒëa 4 ·∫£nh)")
            
            log(f"    ‚úÖ Intelligent collage {num_images} ·∫£nh v·ªõi smart positioning v√† enhanced cropping")
        
        collage = collage.set_duration(duration)
        log(f"    ‚úÖ Collage ho√†n th√†nh ({duration}s)")
        
        # üîß CLEANUP: Close all individual image clips ƒë·ªÉ tr√°nh memory leak
        try:
            for img in images:
                if hasattr(img, 'close'):
                    img.close()
            log(f"    üßπ Cleaned up {len(images)} image clips")
        except Exception as cleanup_error:
            log(f"    ‚ö†Ô∏è Cleanup warning: {str(cleanup_error)}")
        
        return collage
        
    except Exception as e:
        log(f"    ‚ùå L·ªói t·∫°o collage: {str(e)}")
        # Fallback: ch·ªâ d√πng ·∫£nh ƒë·∫ßu ti√™n v·ªõi smart resize
        try:
            img = ImageClip(image_paths[0])
            result = smart_resize_image(img, RESOLUTION, 'smart_crop').set_duration(duration)
            img.close()  # Clean up fallback image
            return result
        except Exception as fallback_error:
            log(f"    ‚ùå Fallback error: {str(fallback_error)}")
            return None

def _process_ai_scoring_and_replacement(preliminary_images, image_files, video_files, selected_videos, find_file_path_func):
    """Helper function for AI scoring and replacement logic (used in RANDOM selection)"""
    selected_images = []
    rejected_images = []
    available_backup_images = [img for img in image_files if img not in preliminary_images]
    
    for i, image_file in enumerate(preliminary_images):
        log(f"üì∏ ƒê√°nh gi√° ·∫£nh {i+1}/{len(preliminary_images)}: {image_file}")
        
        current_image = image_file
        replacement_attempts = 0
        max_attempts = 3  # T·ªëi ƒëa thay th·∫ø 3 l·∫ßn
        
        while replacement_attempts <= max_attempts:
            image_path = find_file_path_func(current_image)
            
            # AI ch·∫•m ƒëi·ªÉm
            total_score, detail, is_acceptable, rejection_reason = calculate_total_image_score(image_path)
            
            log(f"   ƒêi·ªÉm: {total_score:.1f}/100 - {detail}")
            
            if is_acceptable:
                selected_images.append(current_image)
                log(f"   ‚úÖ Ch·∫•p nh·∫≠n: {rejection_reason}")
                break
            else:
                rejected_images.append((current_image, rejection_reason))
                log(f"   ‚ùå T·ª´ ch·ªëi: {rejection_reason}")
                replacement_attempts += 1
                
                # T√¨m ·∫£nh thay th·∫ø
                if replacement_attempts <= max_attempts and available_backup_images:
                    log(f"   üîÑ Th·ª≠ thay th·∫ø l·∫ßn {replacement_attempts}/{max_attempts}")
                    current_image = random.choice(available_backup_images)
                    available_backup_images.remove(current_image)
                    log(f"   üì∏ Th·ª≠ ·∫£nh thay th·∫ø: {current_image}")
                else:
                    # H·∫øt ·∫£nh thay th·∫ø ho·∫∑c ƒë√£ th·ª≠ ƒë·ªß 3 l·∫ßn
                    if replacement_attempts > max_attempts:
                        log(f"   ‚è∞ ƒê√£ th·ª≠ {max_attempts} l·∫ßn, ch·∫•p nh·∫≠n ·∫£nh cu·ªëi")
                        selected_images.append(current_image)
                    else:
                        log(f"   üì≠ H·∫øt ·∫£nh backup, ch·∫•p nh·∫≠n ·∫£nh g·ªëc")
                        selected_images.append(image_file)
                    break
    
    # ƒê·∫£m b·∫£o t·ªëi thi·ªÉu video n·∫øu c√≥
    if len(selected_videos) == 0 and len(video_files) > 0:
        log("Kh√¥ng c√≥ video n√†o ƒë∆∞·ª£c ch·ªçn, th√™m 1 video t·ªëi thi·ªÉu")
        # Thay 1 ·∫£nh b·∫±ng 1 video
        if selected_images:
            selected_images.pop()
        selected_videos.append(video_files[0])
    
    return selected_images, rejected_images

def select_random_materials(image_files, video_files, find_file_path_func, skip_ai_scoring=False):
    """Ch·ªçn nguy√™n li·ªáu ng·∫´u nhi√™n v·ªõi AI ch·∫•m ƒëi·ªÉm v√† l·ªçc ·∫£nh ch·∫•t l∆∞·ª£ng th·∫•p
    Args:
        skip_ai_scoring: N·∫øu True, b·ªè qua AI scoring v√† d√πng h·∫øt files (cho manual selection)
    """
    total_materials = len(image_files) + len(video_files)
    
    # ========== MANUAL SELECTION LOGIC ==========
    if skip_ai_scoring:
        global MANUAL_SELECTED_FILES
        log(f"‚úã MANUAL SELECTION: User ƒë√£ ch·ªçn {len(MANUAL_SELECTED_FILES)} file")
        
        # Ph√¢n lo·∫°i file user ƒë√£ ch·ªçn th√†nh ·∫£nh v√† video
        manual_images = []
        manual_videos = []
        
        for filename in MANUAL_SELECTED_FILES:
            ext = os.path.splitext(filename)[1].lower()
            if ext in ['.jpg', '.jpeg', '.png', '.webp']:
                if filename in image_files:
                    manual_images.append(filename)
                else:
                    log(f"‚ö†Ô∏è ·∫¢nh kh√¥ng t√¨m th·∫•y: {filename}")
            elif ext in ['.mp4', '.mov', '.avi']:
                if filename in video_files:
                    manual_videos.append(filename)
                else:
                    log(f"‚ö†Ô∏è Video kh√¥ng t√¨m th·∫•y: {filename}")
            else:
                log(f"‚ö†Ô∏è File kh√¥ng h·ªó tr·ª£: {filename}")
        
        log(f"‚úÖ MANUAL SELECTION K·∫æT QU·∫¢: {len(manual_images)} ·∫£nh, {len(manual_videos)} video")
        log(f"üìã Danh s√°ch ·∫£nh: {manual_images[:5]}{'...' if len(manual_images) > 5 else ''}")
        log(f"üìã Danh s√°ch video: {manual_videos[:3]}{'...' if len(manual_videos) > 3 else ''}")
        
        # Tr·∫£ v·ªÅ ngay file user ƒë√£ ch·ªçn, kh√¥ng qua AI scoring
        return manual_images, manual_videos
    
    # ========== RANDOM SELECTION LOGIC (C≈®) ==========
    
    # B∆∞·ªõc 1: Logic ch·ªçn s·ªë l∆∞·ª£ng nguy√™n li·ªáu cho random selection
    if not skip_ai_scoring:
        # RANDOM SELECTION: Logic ch·ªçn s·ªë l∆∞·ª£ng nh∆∞ c≈©
        if total_materials < MIN_MATERIALS:
            # N·∫øu t·ªïng kho < 15 ‚Üí l·∫•y h·∫øt
            num_materials = total_materials
            log(f"T·ªïng kho {total_materials} < {MIN_MATERIALS}, l·∫•y h·∫øt")
        elif total_materials < 5:
            # T·ªëi thi·ªÉu 5 nguy√™n li·ªáu ƒë·ªÉ t·∫°o clip
            log(f"‚ö†Ô∏è Ch·ªâ c√≥ {total_materials} nguy√™n li·ªáu < 5, kh√¥ng ƒë·ªß ƒë·ªÉ t·∫°o clip!")
            return [], []
        else:
            # Random 15-30 nguy√™n li·ªáu
            num_materials = random.randint(MIN_MATERIALS, min(MAX_MATERIALS, total_materials))
            log(f"Random ch·ªçn {num_materials} nguy√™n li·ªáu t·ª´ t·ªïng {total_materials} nguy√™n li·ªáu")
    else:
        # MANUAL SELECTION: D√πng h·∫øt t·∫•t c·∫£ files user ƒë√£ ch·ªçn
        num_materials = total_materials
        log(f"‚úã MANUAL SELECTION: D√πng h·∫øt {num_materials} nguy√™n li·ªáu user ƒë√£ ch·ªçn")
    
    # B∆∞·ªõc 2: T·∫°o danh s√°ch t·∫•t c·∫£ nguy√™n li·ªáu
    all_materials = []
    for img in image_files:
        all_materials.append(('image', img))
    for vid in video_files:
        all_materials.append(('video', vid))
    
    # B∆∞·ªõc 3: Random ch·ªçn nguy√™n li·ªáu
    random.shuffle(all_materials)
    selected_materials = all_materials[:num_materials]
    
    # B∆∞·ªõc 4: T√°ch ·∫£nh v√† video
    preliminary_images = [item[1] for item in selected_materials if item[0] == 'image']
    selected_videos = [item[1] for item in selected_materials if item[0] == 'video']
    
    log(f"Ch·ªçn s∆° b·ªô: {len(preliminary_images)} ·∫£nh, {len(selected_videos)} video")
    
    # ============ AI CH·∫§M ƒêI·ªÇM V√Ä L·ªåC ·∫¢NH ============
    if skip_ai_scoring:
        # MANUAL SELECTION: D√πng h·∫øt t·∫•t c·∫£ ·∫£nh user ƒë√£ ch·ªçn, kh√¥ng c·∫ßn AI scoring
        log("‚úã MANUAL SELECTION: B·ªè qua AI ch·∫•m ƒëi·ªÉm, d√πng h·∫øt t·∫•t c·∫£ ·∫£nh user ƒë√£ ch·ªçn")
        selected_images = preliminary_images
        rejected_images = []
        
        # ƒê·∫£m b·∫£o t·ªëi thi·ªÉu video n·∫øu c√≥ (√°p d·ª•ng cho c·∫£ manual v√† random)
        if len(selected_videos) == 0 and len(video_files) > 0:
            log("Kh√¥ng c√≥ video n√†o ƒë∆∞·ª£c ch·ªçn, th√™m 1 video t·ªëi thi·ªÉu")
            # Thay 1 ·∫£nh b·∫±ng 1 video (n·∫øu c√≥ ·∫£nh)
            if selected_images:
                selected_images.pop()
            selected_videos.append(video_files[0])
    else:
        # RANDOM SELECTION: √Åp d·ª•ng AI scoring nh∆∞ c≈©
        log("ü§ñ RANDOM SELECTION: B·∫Øt ƒë·∫ßu AI ch·∫•m ƒëi·ªÉm v√† l·ªçc ·∫£nh...")
        selected_images, rejected_images = _process_ai_scoring_and_replacement(
            preliminary_images, image_files, video_files, selected_videos, find_file_path_func
        )
    
    log(f"K·∫øt qu·∫£ cu·ªëi: {len(selected_images)} ·∫£nh, {len(selected_videos)} video")
    return selected_images, selected_videos

def create_random_image_groups(image_files, max_images=None):
    """T·∫°o c√°c nh√≥m ·∫£nh ng·∫´u nhi√™n"""
    if max_images is None:
        max_images = len(image_files)
    
    # Gi·ªõi h·∫°n s·ªë ·∫£nh n·∫øu c·∫ßn
    if len(image_files) > max_images:
        image_files = random.sample(image_files, max_images)
        log(f"Gi·ªõi h·∫°n {max_images} ·∫£nh t·ª´ {len(image_files)} ·∫£nh c√≥ s·∫µn")
    
    # T·∫°o c√°c nh√≥m ng·∫´u nhi√™n
    groups = []
    remaining_images = image_files.copy()
    
    while remaining_images:
        # Ch·ªçn ng·∫´u nhi√™n s·ªë ·∫£nh trong nh√≥m (1-5)
        max_in_group = min(MAX_IMAGES_PER_FRAME, len(remaining_images))
        group_size = random.randint(1, max_in_group)
        
        # L·∫•y ·∫£nh cho nh√≥m n√†y
        group = remaining_images[:group_size]
        remaining_images = remaining_images[group_size:]
        groups.append(group)
        
        log(f"Nh√≥m {len(groups)}: {group_size} ·∫£nh")
    
    return groups

def select_music(auto_random=False):
    """Cho ph√©p ng∆∞·ªùi d√πng ch·ªçn nh·∫°c ho·∫∑c ch·ªçn ng·∫´u nhi√™n"""
    music_files = list_music_files()
    
    if not music_files:
        log("Kh√¥ng c√≥ file nh·∫°c n√†o!")
        return None
    
    if len(music_files) == 1:
        selected_music = music_files[0]
        log(f"Ch·ªâ c√≥ 1 file nh·∫°c, t·ª± ƒë·ªông ch·ªçn: {selected_music}")
        return os.path.join(MUSIC_FOLDER, selected_music)
    
    # Hi·ªÉn th·ªã danh s√°ch nh·∫°c
    log(f"T√¨m th·∫•y {len(music_files)} file nh·∫°c:")
    for i, music_file in enumerate(music_files, 1):
        log(f"  {i}. {music_file}")
    
    # N·∫øu auto_random=True (t·ª´ web), t·ª± ƒë·ªông ch·ªçn ng·∫´u nhi√™n
    if auto_random:
        selected_music = random.choice(music_files)
        log(f"‚úÖ ƒê√£ ch·ªçn ng·∫´u nhi√™n t·ª± ƒë·ªông: {selected_music}")
        return os.path.join(MUSIC_FOLDER, selected_music)
    
    # H·ªèi ng∆∞·ªùi d√πng ch·ªçn (ch·ªâ khi ch·∫°y t·ª´ command line)
    print("\n[MUSIC] Ch·ªçn file nh·∫°c:")
    print(f"  0. Ch·ªçn ng·∫´u nhi√™n")
    for i, music_file in enumerate(music_files, 1):
        print(f"  {i}. {music_file}")
    
    while True:
        try:
            choice = int(input(f"Nh·∫≠p l·ª±a ch·ªçn (0-{len(music_files)}): "))
            if choice == 0:
                # Ch·ªçn ng·∫´u nhi√™n
                selected_music = random.choice(music_files)
                log(f"‚úÖ ƒê√£ ch·ªçn ng·∫´u nhi√™n: {selected_music}")
                return os.path.join(MUSIC_FOLDER, selected_music)
            elif 1 <= choice <= len(music_files):
                selected_music = music_files[choice - 1]
                log(f"‚úÖ ƒê√£ ch·ªçn: {selected_music}")
                return os.path.join(MUSIC_FOLDER, selected_music)
            else:
                print(f"‚ùå Vui l√≤ng nh·∫≠p s·ªë t·ª´ 0 ƒë·∫øn {len(music_files)}")
        except ValueError:
            print("‚ùå Vui l√≤ng nh·∫≠p m·ªôt s·ªë h·ª£p l·ªá")
        except KeyboardInterrupt:
            log("H·ªßy ch·ªçn nh·∫°c, ch·ªçn ng·∫´u nhi√™n")
            selected_music = random.choice(music_files)
            log(f"‚úÖ ƒê√£ ch·ªçn ng·∫´u nhi√™n: {selected_music}")
            return os.path.join(MUSIC_FOLDER, selected_music)



def create_memories_video(username=None, custom_music_path=None):
    # üïê B·∫ÆT ƒê·∫¶U TIMER CHO TO√ÄN B·ªò QUY TR√åNH
    video_timer.start("T·∫°o Video K·ª∑ Ni·ªám v·ªõi GPU Acceleration")
    
    # Set OUTPUT_FOLDER to user's memories directory
    global OUTPUT_FOLDER
    original_output_folder = OUTPUT_FOLDER
    if username:
        memories_folder = os.path.join('E:', 'EverLiving_UserData', username, 'memories')
        os.makedirs(memories_folder, exist_ok=True)
        OUTPUT_FOLDER = memories_folder
        log(f"‚úÖ Output set to memories folder: {OUTPUT_FOLDER}")
    
    log("B·∫Øt ƒë·∫ßu t·∫°o video k·ª∑ ni·ªám v·ªõi logic m·ªõi")
    log(f"Th∆∞ m·ª•c ·∫£nh/video: {INPUT_FOLDER}")
    log(f"Ch·ªçn ng·∫´u nhi√™n {MIN_MATERIALS}-{MAX_MATERIALS} nguy√™n li·ªáu")
    log(f"Th·ªùi l∆∞·ª£ng m·ª•c ti√™u: {MIN_DURATION}-{TARGET_DURATION}s")
    log(f"Custom music: {custom_music_path if custom_music_path else 'Kh√¥ng c√≥'}")
    
    # B∆Ø·ªöC 0: Ki·ªÉm tra v√† c·∫•u h√¨nh CPU MAXIMUM PERFORMANCE
    video_timer.phase_start("Ki·ªÉm tra v√† t·ªëi ∆∞u CPU")
    log("B∆∞·ªõc 0: C·∫•u h√¨nh CPU maximum performance (44 cores + 128GB RAM)")
    global GPU_ENABLED
    
    # Log CPU status v√† optimization info
    log_cpu_status()
    video_timer.phase_update("CPU 44 cores ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a")
    
    if GPU_ENABLED:
        # GPU v·∫´n c√≥ th·ªÉ ƒë∆∞·ª£c check nh∆∞ng s·∫Ω kh√¥ng d√πng v√¨ ta ƒë√£ t·∫Øt
        gpu_ok = log_gpu_status()
        if gpu_ok:
            log("üöÄ GPU kh·∫£ d·ª•ng nh∆∞ng ƒë√£ DISABLED ƒë·ªÉ d√πng CPU m·∫°nh h∆°n")
        GPU_ENABLED = False  # Force disable GPU ƒë·ªÉ d√πng CPU
        video_timer.phase_update("GPU disabled, s·ª≠ d·ª•ng CPU maximum performance")
    else:
        log("üíª CPU MAXIMUM PERFORMANCE MODE - 44 cores + 128GB RAM")

    # B∆Ø·ªöC 1: Ch·ªçn nh·∫°c tr∆∞·ªõc khi x·ª≠ l√Ω
    log("B∆∞·ªõc 1: Ch·ªçn nh·∫°c n·ªÅn")
    selected_music = None
    
    if custom_music_path:
        # Ki·ªÉm tra n·∫øu l√† URL
        if custom_music_path.startswith(('http://', 'https://')):
            if is_supported_music_url(custom_music_path):
                log(f"üåê ƒêang download nh·∫°c t·ª´ URL: {custom_music_path}")
                # T·∫°o th∆∞ m·ª•c temp cho download
                temp_music_folder = os.path.join(tempfile.gettempdir(), 'everTrace_music')
                os.makedirs(temp_music_folder, exist_ok=True)
                
                # Download audio
                downloaded_path = download_audio_from_url(custom_music_path, temp_music_folder)
                if downloaded_path and os.path.exists(downloaded_path):
                    selected_music = downloaded_path
                    log(f"‚úÖ Download th√†nh c√¥ng: {os.path.basename(selected_music)}")
                else:
                    log(f"‚ùå Kh√¥ng th·ªÉ download nh·∫°c t·ª´ URL")
                    selected_music = select_music(auto_random=True)  # Fallback to random
            else:
                log(f"‚ùå URL kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {custom_music_path}")
                selected_music = select_music(auto_random=True)  # Fallback to random
        elif os.path.exists(custom_music_path):
            selected_music = custom_music_path
            log(f"üéµ S·ª≠ d·ª•ng nh·∫°c local: {os.path.basename(selected_music)}")
        else:
            log(f"‚ùå File nh·∫°c kh√¥ng t·ªìn t·∫°i: {custom_music_path}")
            selected_music = select_music(auto_random=True)  # Fallback to random
    else:
        selected_music = select_music(auto_random=True)
        log(f"üé≤ Ch·ªçn nh·∫°c ng·∫´u nhi√™n: {os.path.basename(selected_music) if selected_music else 'Kh√¥ng c√≥'}")
        
    if selected_music and os.path.exists(selected_music):
        try:
            music_audio = AudioFileClip(selected_music)
            music_duration = music_audio.duration
            log(f"‚úÖ ƒê√£ ch·ªçn nh·∫°c: {os.path.basename(selected_music)}")
            log(f"   Th·ªùi l∆∞·ª£ng nh·∫°c: {music_duration:.1f}s")
            music_audio.close()
            del music_audio
        except Exception as e:
            log(f"‚ùå L·ªói ƒë·ªçc file nh·∫°c: {str(e)}")
            selected_music = None
            music_duration = TARGET_DURATION
    else:
        log("‚ö†Ô∏è Kh√¥ng s·ª≠ d·ª•ng nh·∫°c n·ªÅn")
        selected_music = None
        music_duration = TARGET_DURATION

    # üÜï B∆Ø·ªöC 0.5: X√°c ƒë·ªãnh outro file d·ª±a tr√™n resolution ƒë√£ ch·ªçn
    log("B∆∞·ªõc 0.5: X√°c ƒë·ªãnh outro video")
    if RESOLUTION == RESOLUTION_VERTICAL:
        outro_filename = "outro doc.mp4"
        log(f"‚úÖ Ch·ªçn outro d·ªçc cho {RESOLUTION}")
    elif RESOLUTION == RESOLUTION_SQUARE:
        outro_filename = "outro ngang.mp4"  # C√≥ th·ªÉ t·∫°o outro vu√¥ng ri√™ng sau
        log(f"‚úÖ Ch·ªçn outro vu√¥ng cho {RESOLUTION}")
    else:
        outro_filename = "outro ngang.mp4"
        log(f"‚úÖ Ch·ªçn outro ngang cho {RESOLUTION}")

    # L·∫•y danh s√°ch file (scan subfolders)
    valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.mp4', '.mov', '.avi']
    files = []
    
    # Scan root folder
    if os.path.exists(INPUT_FOLDER):
        for f in os.listdir(INPUT_FOLDER):
            if os.path.splitext(f)[1].lower() in valid_extensions:
                files.append(f)
    
    # Scan images subfolder
    images_folder = os.path.join(INPUT_FOLDER, 'images')
    if os.path.exists(images_folder):
        for f in os.listdir(images_folder):
            if os.path.splitext(f)[1].lower() in valid_extensions:
                files.append(f)
    
    # Scan videos subfolder  
    videos_folder = os.path.join(INPUT_FOLDER, 'videos')
    if os.path.exists(videos_folder):
        for f in os.listdir(videos_folder):
            if os.path.splitext(f)[1].lower() in valid_extensions:
                files.append(f)
    
    import datetime
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    log(f"üîç DEBUG [{timestamp}]: Total files found: {len(files)} in {INPUT_FOLDER} (including subfolders)")
    print(f"üîç DEBUG [{timestamp}]: create_memories_video scanning files...")
    print(f"üîç DEBUG [{timestamp}]: Root folder exists: {os.path.exists(INPUT_FOLDER)}")
    print(f"üîç DEBUG [{timestamp}]: Images folder exists: {os.path.exists(os.path.join(INPUT_FOLDER, 'images'))}")
    print(f"üîç DEBUG [{timestamp}]: Videos folder exists: {os.path.exists(os.path.join(INPUT_FOLDER, 'videos'))}")
    print(f"üîç DEBUG [{timestamp}]: Files list: {files[:10] if files else 'No files found'}")
    
    # Create helper function to find file path in subfolders
    def find_file_path_in_create_memories(filename):
        """Find the full path of a file in INPUT_FOLDER or its subfolders"""
        # Try root folder first
        root_path = os.path.join(INPUT_FOLDER, filename)
        if os.path.exists(root_path):
            return root_path
        
        # Try images subfolder
        images_path = os.path.join(INPUT_FOLDER, 'images', filename)
        if os.path.exists(images_path):
            return images_path
        
        # Try videos subfolder
        videos_path = os.path.join(INPUT_FOLDER, 'videos', filename)
        if os.path.exists(videos_path):
            return videos_path
        
        # If not found anywhere, return original path for error handling
        return root_path
    
    # Store as global for other functions to use
    global find_file_path_in_memories
    find_file_path_in_memories = find_file_path_in_create_memories
    
    if not files:
        raise Exception("Kh√¥ng t√¨m th·∫•y file h·ª£p l·ªá trong th∆∞ m·ª•c!")

    # Ph√¢n lo·∫°i ·∫£nh v√† video
    image_files = [f for f in files if os.path.splitext(f)[1].lower() in ['.jpg', '.jpeg', '.png', '.webp']]
    video_files = [f for f in files if os.path.splitext(f)[1].lower() in ['.mp4', '.mov', '.avi']]

    log(f"Kho nguy√™n li·ªáu: {len(image_files)} ·∫£nh, {len(video_files)} video")

    # Create helper function to find file path in subfolders
    def find_file_path_func(filename):
        """Find the full path of a file in INPUT_FOLDER or its subfolders"""
        # Try root folder first
        root_path = os.path.join(INPUT_FOLDER, filename)
        if os.path.exists(root_path):
            return root_path
        
        # Try images subfolder
        images_path = os.path.join(INPUT_FOLDER, 'images', filename)
        if os.path.exists(images_path):
            return images_path
        
        # Try videos subfolder
        videos_path = os.path.join(INPUT_FOLDER, 'videos', filename)
        if os.path.exists(videos_path):
            return videos_path
        
        # If not found anywhere, return root path (for error handling)
        return root_path

    # B∆Ø·ªöC 1: Ch·ªçn nguy√™n li·ªáu
    video_timer.phase_start("Ch·ªçn nguy√™n li·ªáu v√† AI x·ª≠ l√Ω")
    
    # Ki·ªÉm tra xem c√≥ ph·∫£i manual selection kh√¥ng
    global IS_MANUAL_SELECTION
    skip_ai_scoring = IS_MANUAL_SELECTION
    
    if skip_ai_scoring:
        log("‚úã MANUAL SELECTION: B·∫Øt ƒë·∫ßu v·ªõi file ƒë√£ ch·ªçn t·ª´ user")
    else:
        log("üé≤ RANDOM SELECTION: B·∫Øt ƒë·∫ßu ch·ªçn nguy√™n li·ªáu ng·∫´u nhi√™n")
    
    selected_images, selected_videos = select_random_materials(image_files, video_files, find_file_path_func, skip_ai_scoring=skip_ai_scoring)
    video_timer.phase_update(f"ƒê√£ ch·ªçn {len(selected_images)} ·∫£nh v√† {len(selected_videos)} video")

    clips = []
    total_duration = 0

    # B∆Ø·ªöC 2: T√≠nh to√°n th·ªùi gian v√† x·ª≠ l√Ω video
    video_timer.phase_start("T√≠nh to√°n th·ªùi gian v√† processing")
    log("B∆∞·ªõc 2: X·ª≠ l√Ω video v√† t√≠nh to√°n th·ªùi gian")
    
    video_clips = []
    estimated_video_duration = 0
    video_data = []  # L∆∞u th√¥ng tin video ƒë·ªÉ x·ª≠ l√Ω 2 b∆∞·ªõc
    original_video_clips = []  # L∆∞u original clips ƒë·ªÉ cleanup sau c√πng
    
    # B∆Ø·ªöC 1: X·ª≠ l√Ω c∆° b·∫£n - 1 clip m·ªói video
    video_timer.phase_start("X·ª≠ l√Ω video v√† AI smart cropping")
    log("B∆Ø·ªöC 1: X·ª≠ l√Ω c∆° b·∫£n (1 clip/video)")
    for i, video_file in enumerate(selected_videos):
        video_path = find_file_path_func(video_file)
        log(f"  Video {i+1}: {video_file}")
        
        try:
            video_clip = VideoFileClip(video_path)
            original_duration = video_clip.duration
            original_fps = getattr(video_clip, 'fps', 24)
            log(f"    Th·ªùi l∆∞·ª£ng g·ªëc: {original_duration:.1f}s, FPS: {original_fps}")
            
            if original_duration >= 5.0:  # Video >= 5s m·ªõi x·ª≠ l√Ω
                # L∆∞u th√¥ng tin video ƒë·ªÉ x·ª≠ l√Ω b∆∞·ªõc 2 (gi·ªØ reference ƒë·∫øn clip)
                video_data.append({
                    'file': video_file,
                    'path': video_path,
                    'clip': video_clip,  # Gi·ªØ reference ƒë·ªÉ tr√°nh l·ªói get_frame
                    'duration': original_duration,
                    'used_segments': []  # Track c√°c ƒëo·∫°n ƒë√£ c·∫Øt
                })
                
                # L∆∞u clip ƒë·ªÉ cleanup sau c√πng
                original_video_clips.append(video_clip)
                if original_duration > MAX_VIDEO_CLIP_DURATION:
                    # Video d√†i: C·∫Øt 1 clip random 8-12s
                    clip_duration = random.uniform(MIN_VIDEO_CLIP_DURATION, MAX_VIDEO_CLIP_DURATION)
                    max_start = original_duration - clip_duration
                    start_time = random.uniform(0, max_start)
                    end_time = min(start_time + clip_duration, original_duration)
                    
                    # C·∫Øt clip
                    sub_clip = video_clip.subclip(start_time, end_time)
                    sub_clip = smart_resize_video(sub_clip, RESOLUTION, FPS)
                    
                    video_clips.append(sub_clip)
                    estimated_video_duration += (end_time - start_time)
                    
                    # L∆∞u segment ƒë√£ d√πng
                    video_data[-1]['used_segments'].append((start_time, end_time))
                    
                    log(f"    Clip {i+1}.1: {start_time:.1f}s-{end_time:.1f}s ({end_time-start_time:.1f}s)")
                else:
                    # Video ng·∫Øn: D√πng nguy√™n
                    sub_clip = smart_resize_video(video_clip, RESOLUTION, FPS)
                    video_clips.append(sub_clip)
                    estimated_video_duration += original_duration
                    
                    # ƒê√°nh d·∫•u to√†n b·ªô video ƒë√£ d√πng
                    video_data[-1]['used_segments'].append((0, original_duration))
                    
                    log(f"    Clip {i+1}.1: D√πng nguy√™n ({original_duration:.1f}s)")
            else:
                log(f"    ‚ùå Video qu√° ng·∫Øn ({original_duration:.1f}s < 5s), b·ªè qua")
                # Cleanup ngay video kh√¥ng d√πng ƒë∆∞·ª£c
                original_video_clips.append(video_clip)
                
        except Exception as e:
            log(f"    ‚ùå L·ªói x·ª≠ l√Ω video: {str(e)}")
            # Cleanup clip n·∫øu c√≥ l·ªói v√† ƒë√£ t·∫°o
            try:
                if 'video_clip' in locals() and video_clip is not None:
                    original_video_clips.append(video_clip)  # V·∫´n add ƒë·ªÉ cleanup
            except:
                pass
    
    log(f"T·ªïng B∆∞·ªõc 1: {estimated_video_duration:.1f}s (< {MAX_VIDEO_MATERIALS_DURATION}s)")
    
    # ƒê·∫øm s·ªë video th·ª±c t·∫ø ƒë∆∞·ª£c x·ª≠ l√Ω sau B∆∞·ªõc 1
    processed_videos = len([v for v in video_data if len(v['used_segments']) > 0])
    
    # B∆Ø·ªöC 2: T·ªëi ∆∞u h√≥a - C·∫Øt th√™m n·∫øu c√≤n d∆∞ th·ªùi gian
    if estimated_video_duration < MAX_VIDEO_MATERIALS_DURATION - MIN_VIDEO_CLIP_DURATION:
        remaining_time = MAX_VIDEO_MATERIALS_DURATION - estimated_video_duration
        log(f"B∆Ø·ªöC 2: T·ªëi ∆∞u h√≥a (c√≤n d∆∞ {remaining_time:.1f}s)")
        
        clip_counter_per_video = {}  # Track s·ªë clip cho m·ªói video
        
        # Duy·ªát c√°c video ƒë·ªÉ c·∫Øt th√™m
        for video_info in video_data:
            if estimated_video_duration >= MAX_VIDEO_MATERIALS_DURATION - MIN_VIDEO_CLIP_DURATION:
                break
            
            # üîß S·ª¨ D·ª§NG: D√πng clip ƒë√£ load thay v√¨ reload
            video_clip = video_info['clip']  # D√πng clip ƒë√£ c√≥
            original_duration = video_info['duration']
            used_segments = video_info['used_segments']
            video_name = video_info['file']
            
            # T√¨m v√πng ch∆∞a s·ª≠ d·ª•ng
            available_segments = []
            current_pos = 0
            
            # S·∫Øp x·∫øp used_segments theo th·ªùi gian
            used_segments.sort(key=lambda x: x[0])
            
            for start, end in used_segments:
                if current_pos < start - VIDEO_CLIP_GAP:
                    available_segments.append((current_pos, start - VIDEO_CLIP_GAP))
                current_pos = max(current_pos, end + VIDEO_CLIP_GAP)
            
            # Th√™m ph·∫ßn cu·ªëi n·∫øu c√≥
            if current_pos < original_duration:
                available_segments.append((current_pos, original_duration))
            
            # C·∫Øt th√™m t·ª´ c√°c segment available
            for avail_start, avail_end in available_segments:
                if estimated_video_duration >= MAX_VIDEO_MATERIALS_DURATION - MIN_VIDEO_CLIP_DURATION:
                    break
                    
                avail_duration = avail_end - avail_start
                if avail_duration >= MIN_VIDEO_CLIP_DURATION:
                    # Random clip duration
                    clip_duration = min(
                        random.uniform(MIN_VIDEO_CLIP_DURATION, MAX_VIDEO_CLIP_DURATION),
                        avail_duration
                    )
                    
                    # Ki·ªÉm tra xem th√™m clip n√†y c√≥ v∆∞·ª£t qu√° kh√¥ng
                    if estimated_video_duration + clip_duration <= MAX_VIDEO_MATERIALS_DURATION:
                        # Random start position trong available segment
                        max_start = avail_end - clip_duration
                        start_time = random.uniform(avail_start, max_start)
                        end_time = min(start_time + clip_duration, avail_end)
                        
                        # C·∫Øt clip b·ªï sung
                        sub_clip = video_clip.subclip(start_time, end_time)
                        sub_clip = smart_resize_video(sub_clip, RESOLUTION, FPS)
                        
                        video_clips.append(sub_clip)
                        estimated_video_duration += (end_time - start_time)
                        
                        # C·∫≠p nh·∫≠t used_segments
                        video_info['used_segments'].append((start_time, end_time))
                        
                        video_index = video_data.index(video_info) + 1
                        if video_index not in clip_counter_per_video:
                            clip_counter_per_video[video_index] = 2  # B·∫Øt ƒë·∫ßu t·ª´ .2
                        
                        clip_number = clip_counter_per_video[video_index]
                        log(f"  + Video {video_index}: Clip {video_index}.{clip_number} ({start_time:.1f}s-{end_time:.1f}s) = {end_time-start_time:.1f}s ‚Üí T·ªïng: {estimated_video_duration:.1f}s")
                        
                        clip_counter_per_video[video_index] += 1
                    else:
                        log(f"  + Video {video_data.index(video_info) + 1}: Clip s·∫Ω v∆∞·ª£t {MAX_VIDEO_MATERIALS_DURATION}s, b·ªè qua")
                        break
        
        log(f"T·ªïng cu·ªëi: {estimated_video_duration:.1f}s t·ª´ {len(video_clips)} clips")
        
        # C·∫≠p nh·∫≠t s·ªë video th·ª±c t·∫ø ƒë∆∞·ª£c x·ª≠ l√Ω (c√≥ th·ªÉ thay ƒë·ªïi sau B∆∞·ªõc 2)
        processed_videos = len([v for v in video_data if len(v['used_segments']) > 0])
        log(f"‚Ä¢ K·∫æT QU·∫¢: {len(video_clips)} clips t·ª´ {processed_videos} video nguy√™n li·ªáu")
        log(f"‚Ä¢ TH·ªúI L∆Ø·ª¢NG VIDEO: {estimated_video_duration:.1f}s/{MAX_VIDEO_MATERIALS_DURATION}s")
    else:
        # N·∫øu kh√¥ng c√≥ B∆Ø·ªöC 2, v·∫´n c·∫ßn log k·∫øt qu·∫£
        log(f"‚Ä¢ K·∫æT QU·∫¢: {len(video_clips)} clips t·ª´ {processed_videos} video nguy√™n li·ªáu")
        log(f"‚Ä¢ TH·ªúI L∆Ø·ª¢NG VIDEO: {estimated_video_duration:.1f}s/{MAX_VIDEO_MATERIALS_DURATION}s")
    
    # ƒê·∫øm s·ªë video b·ªã lo·∫°i b·ªè v√† b√π nguy√™n li·ªáu
    removed_videos_count = len(selected_videos) - processed_videos
    
    if removed_videos_count > 0:
        log(f"üí° B·ªã lo·∫°i {removed_videos_count} video, random b√π th√™m nguy√™n li·ªáu")
        
        # T·∫°o danh s√°ch nguy√™n li·ªáu d·ª± ph√≤ng (ch∆∞a ch·ªçn)
        unused_materials = []
        for img in image_files:
            if img not in selected_images:
                unused_materials.append(('image', img))
        for vid in video_files:
            if vid not in selected_videos:
                unused_materials.append(('video', vid))
        
        # Random b√π nguy√™n li·ªáu
        if unused_materials:
            random.shuffle(unused_materials)
            backup_materials = unused_materials[:removed_videos_count]
            
            for material_type, material_file in backup_materials:
                if material_type == 'image':
                    selected_images.append(material_file)
                    log(f"   + B√π th√™m ·∫£nh: {material_file}")
                else:
                    # Th√™m video v√†o danh s√°ch ƒë·ªÉ x·ª≠ l√Ω l·∫°i
                    selected_videos.append(material_file)
                    log(f"   + B√π th√™m video: {material_file}")

    log(f"T·ªïng th·ªùi l∆∞·ª£ng video: {estimated_video_duration:.1f}s t·ª´ {len(video_clips)} clip (Max: {MAX_VIDEO_MATERIALS_DURATION}s)")
    
    # B∆Ø·ªöC 3: X·ª≠ l√Ω ·∫£nh v·ªõi logic ƒë√∫ng
    video_timer.phase_start("X·ª≠ l√Ω ·∫£nh v·ªõi AI face detection")
    log("B∆∞·ªõc 3: X·ª≠ l√Ω ·∫£nh")
    
    remaining_time = TARGET_DURATION - estimated_video_duration
    remaining_time = max(13, min(remaining_time, 44))  # Gi·ªõi h·∫°n 13-44s cho ·∫£nh (kh√¥i ph·ª•c gi√° tr·ªã g·ªëc)
    
    log(f"Th·ªùi gian c√≤n l·∫°i cho ·∫£nh: {remaining_time:.1f}s ({len(selected_images)} ·∫£nh)")
    
    if selected_images:
        # T√≠nh th·ªùi l∆∞·ª£ng m·ªói ·∫£nh n·∫øu l√†m ƒë∆°n
        time_per_single_image = remaining_time / len(selected_images)
        
        # Ki·ªÉm tra xem c√≥ th·ªÉ l√†m ·∫£nh ƒë∆°n kh√¥ng (3-5s/·∫£nh)
        if 3.0 <= time_per_single_image <= 5.0:
            # ƒê·ªß th·ªùi gian, l√†m ·∫£nh ƒë∆°n
            log(f"ƒê·ªß th·ªùi gian ({time_per_single_image:.1f}s/·∫£nh), t·∫°o ·∫£nh ƒë∆°n")
            
            for i, image_file in enumerate(selected_images):
                image_path = find_file_path_func(image_file)
                single_clip = create_image_collage([image_path], time_per_single_image)
                
                clips.append(single_clip)
                total_duration += time_per_single_image
                
                log(f"  => ·∫¢nh ƒë∆°n {i+1}: {image_file} ({time_per_single_image:.1f}s)")
        
        else:
            # C·∫ßn gh√©p ·∫£nh ƒë·ªÉ ƒë·∫°t 3-5s/clip
            log(f"C·∫ßn gh√©p ·∫£nh (th·ªùi gian/·∫£nh ƒë∆°n: {time_per_single_image:.1f}s)")
            
            if time_per_single_image < 3.0:
                # Thu·∫≠t to√°n Random Grouping v·ªõi Random Duration (3-5s)
                log(f"Thu·∫≠t to√°n random grouping (< 3s/·∫£nh):")
                
                remaining_images = selected_images.copy()  # Copy ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªõng g·ªëc
                collage_clips = []
                clip_durations = []
                total_used_time = 0
                iteration = 1
                
                # B∆Ø·ªöC 1-3: Ba clip ƒë·∫ßu ti√™n B·∫ÆT BU·ªòC ch·ªâ 1 ·∫£nh
                first_clips = 0
                while len(remaining_images) > 0 and first_clips < 3:  # 3 clip ƒë·∫ßu
                    # Random th·ªùi gian cho clip (3-5s)
                    clip_duration = random.uniform(3.0, 5.0)
                    
                    # Ch·ªçn random 1 ·∫£nh duy nh·∫•t
                    selected_image = random.choice(remaining_images)
                    remaining_images.remove(selected_image)
                    total_used_time += clip_duration
                    
                    # Log clip
                    log(f"   ‚≠ê Clip {iteration} (B·∫ÆT BU·ªòC 1 ·∫£nh): ({clip_duration:.2f}s) {os.path.basename(selected_image)}")
                    log(f"   ‚Üí C√≤n l·∫°i: {len(remaining_images)} ·∫£nh, {remaining_time - total_used_time:.1f}s")
                    
                    # L∆∞u clip
                    collage_clips.append([selected_image])
                    clip_durations.append(clip_duration)
                    iteration += 1
                    first_clips += 1
                
                # B∆Ø·ªöC 4+: C√°c clip sau random 1-4 ·∫£nh (thay v√¨ 2-5)
                while len(remaining_images) > 0:
                    remaining_time_left = remaining_time - total_used_time
                    
                    # Random th·ªùi gian (3-5s)
                    clip_duration = random.uniform(3.0, min(5.0, remaining_time_left))
                    
                    # Random s·ªë ·∫£nh (1-4) thay v√¨ (2-5)
                    max_group_size = min(4, len(remaining_images))
                    min_group_size = min(1, len(remaining_images))
                    group_size = random.randint(min_group_size, max_group_size)
                    
                    # Random ch·ªçn ·∫£nh
                    selected_for_group = random.sample(remaining_images, group_size)
                    
                    # Log
                    log(f"   üé≤ Clip {iteration} (RANDOM 1-4): {group_size} ·∫£nh ({clip_duration:.2f}s) {[os.path.basename(img) for img in selected_for_group]}")
                    
                    # L∆∞u clip
                    collage_clips.append(selected_for_group)
                    clip_durations.append(clip_duration)
                    total_used_time += clip_duration
                    
                    # Lo·∫°i b·ªè ·∫£nh ƒë√£ ch·ªçn
                    for img in selected_for_group:
                        remaining_images.remove(img)
                    
                    log(f"   ‚Üí C√≤n l·∫°i: {len(remaining_images)} ·∫£nh, {remaining_time - total_used_time:.1f}s")
                    iteration += 1
                    
                    # Ki·ªÉm tra th·ªùi gian - n·∫øu g·∫ßn h·∫øt th√¨ d·ª´ng
                    if remaining_time - total_used_time < 2.0:
                        if len(remaining_images) > 0:
                            # Fallback: l·∫•y h·∫øt ·∫£nh c√≤n l·∫°i v·ªõi th·ªùi gian c√≤n l·∫°i
                            fallback_duration = remaining_time - total_used_time
                            log(f"   ‚Üí Fallback: L·∫•y h·∫øt {len(remaining_images)} ·∫£nh c√≤n l·∫°i ({fallback_duration:.2f}s)")
                            log(f"   üîö Clip {iteration} (FALLBACK): {len(remaining_images)} ·∫£nh ({fallback_duration:.2f}s) {[os.path.basename(img) for img in remaining_images]}")
                            
                            collage_clips.append(remaining_images.copy())
                            clip_durations.append(fallback_duration)
                            remaining_images.clear()
                        break
                
                # B∆Ø·ªöC 3: Clip 4 tr·ªü ƒëi d√πng logic ki·ªÉm tra ng∆∞·ª£c
                while len(remaining_images) > 0:
                    remaining_time_left = remaining_time - total_used_time
                    
                    # Random th·ªùi gian cho clip ti·∫øp theo (3-5s)
                    if remaining_time_left <= 5.0:
                        clip_duration = remaining_time_left
                    else:
                        clip_duration = random.uniform(3.0, min(5.0, remaining_time_left))
                    
                    # B·∫Øt ƒë·∫ßu v·ªõi s·ªë ·∫£nh t·ªëi thi·ªÉu (2 ·∫£nh) v√† tƒÉng d·∫ßn n·∫øu c·∫ßn
                    found_valid_group = False
                    for group_size in range(2, min(6, len(remaining_images) + 1)):  # 2-5 ·∫£nh
                        
                        # T√≠nh th·ªùi gian c√≤n l·∫°i sau khi t·∫°o clip n√†y
                        images_after = len(remaining_images) - group_size
                        time_after = remaining_time_left - clip_duration
                        
                        # Ki·ªÉm tra: ·∫¢nh c√≤n l·∫°i c√≥ ƒë·ªß th·ªùi gian kh√¥ng?
                        if images_after == 0:
                            # Clip cu·ªëi c√πng, l·∫•y h·∫øt ·∫£nh c√≤n l·∫°i
                            group_size = len(remaining_images)
                            found_valid_group = True
                            log(f"   ‚Üí Clip cu·ªëi: {group_size} ·∫£nh, d√πng h·∫øt th·ªùi gian ({clip_duration:.2f}s)")
                            break
                        elif time_after / images_after >= 1.8:
                            # ·∫¢nh c√≤n l·∫°i ƒë·ªß th·ªùi gian (‚â•1.8s/·∫£nh)
                            found_valid_group = True
                            log(f"   ‚Üí Test {group_size} ·∫£nh: OK ({images_after} ·∫£nh c√≤n l·∫°i, {time_after/images_after:.1f}s/·∫£nh)")
                            break
                        else:
                            # Kh√¥ng ƒë·ªß th·ªùi gian, th·ª≠ v·ªõi nh√≥m l·ªõn h∆°n
                            log(f"   ‚Üí Test {group_size} ·∫£nh: FAIL ({images_after} ·∫£nh c√≤n l·∫°i, {time_after/images_after:.1f}s/·∫£nh < 1.8s)")
                            continue
                    
                    # N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c group h·ª£p l·ªá, l·∫•y h·∫øt ·∫£nh c√≤n l·∫°i
                    if not found_valid_group:
                        group_size = len(remaining_images)
                        clip_duration = remaining_time_left
                        log(f"   ‚Üí Fallback: L·∫•y h·∫øt {group_size} ·∫£nh c√≤n l·∫°i ({clip_duration:.2f}s)")
                    
                    # Random ch·ªçn ·∫£nh (kh√¥ng theo th·ª© t·ª±)
                    selected_for_group = random.sample(remaining_images, group_size)
                    
                    # Log iteration
                    log(f"   üîç Clip {iteration} (CHECK): {group_size} ·∫£nh ({clip_duration:.2f}s) {[os.path.basename(img) for img in selected_for_group]}")
                    
                    # L∆∞u clip v√† th·ªùi gian
                    collage_clips.append(selected_for_group)
                    clip_durations.append(clip_duration)
                    total_used_time += clip_duration
                    
                    # Lo·∫°i b·ªè ·∫£nh ƒë√£ ch·ªçn kh·ªèi danh s√°ch c√≤n l·∫°i
                    for img in selected_for_group:
                        remaining_images.remove(img)
                    
                    iteration += 1
                    
                    # Debug: Ki·ªÉm tra ·∫£nh c√≤n l·∫°i
                    log(f"   ‚Üí C√≤n l·∫°i: {len(remaining_images)} ·∫£nh, {remaining_time_left - clip_duration:.1f}s")
                
                # Ki·ªÉm tra n·∫øu c√≤n ·∫£nh ch∆∞a x·ª≠ l√Ω
                if len(remaining_images) > 0:
                    log(f"‚ö†Ô∏è C·∫¢NH B√ÅO: C√≤n {len(remaining_images)} ·∫£nh ch∆∞a x·ª≠ l√Ω!")
                    for img in remaining_images:
                        log(f"   - {os.path.basename(img)}")
                
                log(f"T·∫°o {len(collage_clips)} clips v·ªõi t·ªïng th·ªùi gian {total_used_time:.2f}s")
                
                # T·∫°o clips t·ª´ groups v·ªõi th·ªùi gian t∆∞∆°ng ·ª©ng
                clips_with_duration = []
                for i, (group, duration) in enumerate(zip(collage_clips, clip_durations)):
                    group_paths = [find_file_path_func(img) for img in group]
                    collage_clip = create_image_collage(group_paths, duration)
                    clips_with_duration.append((collage_clip, duration, len(group)))
                    clips.append(collage_clip)
                    total_duration += duration
                
                # B∆∞·ªõc 2: X√°o tr·ªôn th·ª© t·ª± clips (Double Randomness)
                log(f"X√°o tr·ªôn th·ª© t·ª± clips...")
                random.shuffle(clips_with_duration)
                
                # C·∫≠p nh·∫≠t l·∫°i clips sau khi x√°o tr·ªôn
                clips = clips[-len(clips_with_duration):]  # X√≥a clips c≈©
                clips.clear()
                for clip_info in clips_with_duration:
                    clips.append(clip_info[0])
                
                # Log k·∫øt qu·∫£ sau x√°o tr·ªôn
                for i, (clip, duration, group_size) in enumerate(clips_with_duration):
                    log(f"  => Clip gh√©p {i+1}: {group_size} ·∫£nh ({duration:.2f}s) ‚úÖ")
                
                log(f"‚úÖ X·ª≠ l√Ω h·∫øt {len(selected_images)} ·∫£nh, 2 l·∫ßn random (duration + grouping + order)")
            
            else:
                # Th·ª´a th·ªùi gian -> m·ªôt s·ªë ·∫£nh ƒë∆°n + m·ªôt s·ªë gh√©p
                # ∆Øu ti√™n ·∫£nh ƒë∆°n, gh√©p ph·∫ßn c√≤n l·∫°i n·∫øu c·∫ßn
                single_count = int(remaining_time / 5.0)  # ·∫¢nh ƒë∆°n 5s
                single_count = min(single_count, len(selected_images))
                
                if single_count > 0:
                    remaining_images = selected_images[single_count:]
                    single_time = 5.0
                    remaining_after_singles = remaining_time - (single_count * single_time)
                    
                    # T·∫°o ·∫£nh ƒë∆°n
                    for i in range(single_count):
                        image_path = find_file_path_func(selected_images[i])
                        single_clip = create_image_collage([image_path], single_time)
                        clips.append(single_clip)
                        total_duration += single_time
                        log(f"  => ·∫¢nh ƒë∆°n {i+1}: {selected_images[i]} ({single_time:.1f}s)")
                    
                    # Gh√©p ph·∫ßn c√≤n l·∫°i n·∫øu c√≥ v√† ƒë·ªß th·ªùi gian
                    if remaining_images and remaining_after_singles >= 1.8:
                        group_paths = [find_file_path_func(img) for img in remaining_images]
                        collage_clip = create_image_collage(group_paths, remaining_after_singles)
                        clips.append(collage_clip)
                        total_duration += remaining_after_singles
                        log(f"  => Nh√≥m gh√©p: {len(remaining_images)} ·∫£nh ({remaining_after_singles:.1f}s)")

    # T√≠nh t·ªïng th·ªùi l∆∞·ª£ng th·ª±c t·∫ø (video + image)
    video_duration = sum(clip.duration for clip in video_clips)
    image_duration = sum(clip.duration for clip in clips)
    actual_total_duration = video_duration + image_duration
    
    # Log chi ti·∫øt t·ª´ng ph·∫ßn
    log(f"üìä CHI TI·∫æT TH·ªúI L∆Ø·ª¢NG:")
    log(f"   ‚Ä¢ T·ªïng th·ªùi l∆∞·ª£ng c√°c clip gh√©p t·ª´ h√¨nh ·∫£nh: {image_duration:.1f}s")
    log(f"   ‚Ä¢ T·ªïng th·ªùi l∆∞·ª£ng c√°c video c·∫Øt ra: {video_duration:.1f}s ({len(video_clips)} video)")
    log(f"   ‚Ä¢ T·ªîNG C·ªòNG: {actual_total_duration:.1f}s")

    # B∆Ø·ªöC 4: X√°o tr·ªôn v√† gh√©p video cu·ªëi c√πng
    video_timer.phase_start("Gh√©p video v√† render GPU")
    log("B∆∞·ªõc 4: X√°o tr·ªôn v√† t·∫°o video cu·ªëi c√πng")
    
    if not clips and not video_clips:
        raise Exception("Kh√¥ng c√≥ clip n√†o ƒë·ªÉ t·∫°o video!")
    
    # Merge t·∫•t c·∫£ clips (video + image) ƒë·ªÉ x·ª≠ l√Ω chung
    all_clips = video_clips + clips
    
    # X√°o tr·ªôn t·∫•t c·∫£ clips ƒë·ªÉ t·∫°o s·ª± ng·∫´u nhi√™n
    random.shuffle(all_clips)
    log(f"ƒê√£ x√°o tr·ªôn {len(all_clips)} clips ({len(video_clips)} video + {len(clips)} image)")
    
    # Log th·ªùi l∆∞·ª£ng th·ª±c t·∫ø tr∆∞·ªõc khi apply transitions
    total_duration_before_transitions = sum(clip.duration for clip in all_clips)
    log(f"T·ªïng th·ªùi l∆∞·ª£ng tr∆∞·ªõc transitions: {total_duration_before_transitions:.1f}s")
    
    # √Åp d·ª•ng hi·ªáu ·ª©ng chuy·ªÉn c·∫£nh cho t·∫•t c·∫£ clips
    all_clips = apply_transitions_to_clips(all_clips)
    
    # Gh√©p t·∫•t c·∫£ clips l·∫°i
    final_clip = concatenate_videoclips(all_clips, method="compose")
    actual_final_duration = final_clip.duration
    log(f"ƒê√£ gh√©p video v·ªõi transitions, th·ªùi l∆∞·ª£ng: {actual_final_duration:.1f}s")

    # üÜï B∆Ø·ªöC 5: Th√™m outro video tr∆∞·ªõc khi th√™m nh·∫°c
    log("B∆∞·ªõc 5: Th√™m outro video")
    final_clip = add_outro_video(final_clip, outro_filename)
    log(f"‚úÖ Video sau outro: {final_clip.duration:.1f}s")

    # B∆Ø·ªöC 5.5: Th√™m nh·∫°c n·ªÅn ƒë·ªÉ cover to√†n b·ªô video (bao g·ªìm outro)
    log("B∆∞·ªõc 5.5: Th√™m nh·∫°c n·ªÅn v·ªõi fade effects (cover to√†n b·ªô video + outro)")
    
    if selected_music and os.path.exists(selected_music):
        try:
            # T·∫£i audio
            music_audio = AudioFileClip(selected_music)
            
            # C·∫Øt audio cho ph√π h·ª£p v·ªõi video FINAL (bao g·ªìm outro)
            if music_audio.duration > final_clip.duration:
                music_audio = music_audio.subclip(0, final_clip.duration)
            else:
                # L·∫∑p l·∫°i nh·∫°c n·∫øu ng·∫Øn h∆°n video final (bao g·ªìm outro)
                repeat_times = int(final_clip.duration / music_audio.duration) + 1
                music_clips = [music_audio] * repeat_times
                music_audio = concatenate_audioclips(music_clips).subclip(0, final_clip.duration)
            
            # Th√™m fade in/out effects (fadeout s·∫Ω sync v·ªõi outro)
            music_audio = music_audio.audio_fadein(AUDIO_FADEIN).audio_fadeout(AUDIO_FADEOUT)
            
            # √Åp d·ª•ng audio v√†o video
            final_clip = final_clip.set_audio(music_audio)
            
            log(f"‚úÖ ƒê√£ th√™m nh·∫°c cover to√†n b·ªô video: {os.path.basename(selected_music)}")
            log(f"   Video duration: {final_clip.duration:.1f}s (bao g·ªìm outro)")
            log(f"   Fade in: {AUDIO_FADEIN}s, Fade out: {AUDIO_FADEOUT}s")
            
        except Exception as e:
            log(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω audio (ti·∫øp t·ª•c kh√¥ng c√≥ nh·∫°c): {str(e)}")
            # Ti·∫øp t·ª•c xu·∫•t video kh√¥ng c√≥ nh·∫°c n·∫øu c√≥ l·ªói
    else:
        log("üéµ Kh√¥ng c√≥ nh·∫°c n·ªÅn")

    # B∆Ø·ªöC 6: Xu·∫•t video
    log("B∆∞·ªõc 6: Xu·∫•t video cu·ªëi c√πng")
    print(f"üîç DEBUG: OUTPUT_FOLDER = {OUTPUT_FOLDER}", flush=True)
    print(f"üîç DEBUG: OUTPUT_FILENAME = {OUTPUT_FILENAME}", flush=True)
    output_path = os.path.join(OUTPUT_FOLDER, OUTPUT_FILENAME)
    print(f"üîç DEBUG: Final output_path = {output_path}", flush=True)
    
    try:
        # üîß Safety check: ƒê·∫£m b·∫£o final clip c√≥ audio h·ª£p l·ªá
        if final_clip.audio is not None:
            log("Audio track ƒë∆∞·ª£c ph√°t hi·ªán, ƒëang xu·∫•t video v·ªõi √¢m thanh...")
        else:
            log("‚ö†Ô∏è Kh√¥ng c√≥ audio track, xu·∫•t video silent...")
            
        # Export v·ªõi GPU acceleration cho t·ªëc ƒë·ªô t·ªëi ∆∞u
        if GPU_ENABLED:
            log("üöÄ Xu·∫•t video v·ªõi NVIDIA GPU acceleration (h264_nvenc)...")
            final_clip.write_videofile(
                output_path,
                fps=FPS,
                codec=GPU_CODEC,
                audio_codec=AUDIO_CODEC if final_clip.audio is not None else None,
                ffmpeg_params=GPU_EXTRA_PARAMS,
                logger=None,
                temp_audiofile="temp-audio.m4a",  # Tr√°nh conflict audio
                remove_temp=True
            )
            log(f"‚úÖ GPU Render ho√†n th√†nh! Video ƒë√£ l∆∞u: {output_path}")
            print(f"üîç DEBUG: Video file created at: {output_path}", flush=True)
            print(f"üîç DEBUG: File exists? {os.path.exists(output_path)}", flush=True)
            log(f"   GPU: NVIDIA GTX 1660 with CUDA acceleration")
            log(f"   Settings: {FPS}fps, {GPU_CODEC}, CRF {GPU_CRF}, preset {GPU_PRESET}")
        else:
            log("üöÄ Xu·∫•t video v·ªõi CPU MAXIMUM PERFORMANCE (44 cores + 128GB RAM)...")
            final_clip.write_videofile(
                output_path,
                fps=FPS,
                codec=CPU_CODEC,
                audio_codec=AUDIO_CODEC if final_clip.audio is not None else None,
                ffmpeg_params=CPU_EXTRA_PARAMS,
                threads=THREADS,
                logger=None,
                temp_audiofile="temp-audio.m4a",  # Tr√°nh conflict audio
                remove_temp=True
            )
            log(f"‚úÖ CPU MAXIMUM Render ho√†n th√†nh! Video ƒë√£ l∆∞u: {output_path}")
            log(f"   CPU: 44 cores @ maximum performance v·ªõi 128GB RAM")
            log(f"   Settings: {FPS}fps, {CPU_CODEC}, CRF {CPU_CRF}, preset {CPU_PRESET}")
            log(f"   Bitrate: {CPU_BITRATE}, Quality: CRF {CPU_CRF} (very high quality)")
            
        log(f"   Th·ªùi l∆∞·ª£ng cu·ªëi c√πng: {final_clip.duration:.1f}s")
        
    except Exception as e:
        log(f"‚ùå L·ªói xu·∫•t video: {str(e)}")
        raise
    finally:
        # üõ†Ô∏è PROPER CLEANUP ƒë·ªÉ tr√°nh l·ªói Windows handle
        try:
            # Close t·∫•t c·∫£ clips ƒë·ªÉ gi·∫£i ph√≥ng resources
            if 'final_clip' in locals():
                final_clip.close()
            if 'all_clips' in locals():
                for clip in all_clips:
                    try:
                        clip.close()
                    except:
                        pass
            if 'video_clips' in locals():
                for clip in video_clips:
                    try:
                        clip.close()
                    except:
                        pass
            if 'clips' in locals():
                for clip in clips:
                    try:
                        clip.close()
                    except:
                        pass
            # üÜï CLEANUP: Original video clips sau khi xu·∫•t xong
            if 'original_video_clips' in locals():
                for clip in original_video_clips:
                    try:
                        clip.close()
                    except:
                        pass
                log(f"üßπ Cleaned up {len(original_video_clips)} original video clips")
            if 'music_audio' in locals():
                try:
                    music_audio.close()
                except:
                    pass
            log("üßπ ƒê√£ cleanup resources th√†nh c√¥ng")
            
            # üîß FORCE GARBAGE COLLECTION ƒë·ªÉ gi·∫£i ph√≥ng memory
            import gc
            gc.collect()
            log("üóëÔ∏è Forced garbage collection completed")
            
        except Exception as cleanup_error:
            log(f"‚ö†Ô∏è Cleanup warning (kh√¥ng ·∫£nh h∆∞·ªüng): {cleanup_error}")
            pass  # Kh√¥ng raise l·ªói cleanup
    
    # üèÅ K·∫æT TH√öC V√Ä B√ÅO C√ÅO TH·ªúI GIAN
    total_time = video_timer.finish()
    log(f"üé¨ Video ho√†n th√†nh sau {total_time:.1f} gi√¢y v·ªõi GPU acceleration!")
    
    # Restore original OUTPUT_FOLDER
    OUTPUT_FOLDER = original_output_folder
    
    # Return success result for app_simple.py
    return {
        'success': True,
        'message': 'Video k·ª∑ ni·ªám ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!',
        'filename': os.path.basename(output_path) if 'output_path' in locals() else 'video.mp4',
        'video_path': output_path if 'output_path' in locals() else ''
    }


def create_video_from_images(user_folder, video_format, selected_files, username, music_path=None):
    """
    T·∫°o video t·ª´ ·∫£nh ƒë√£ ch·ªçn cho web app
    Args:
        user_folder: Th∆∞ m·ª•c ch·ª©a ·∫£nh c·ªßa user
        video_format: ƒê·ªãnh d·∫°ng video (horizontal/vertical/square)
        selected_files: List c√°c file ƒë√£ ch·ªçn
        username: T√™n user
        music_path: ƒê∆∞·ªùng d·∫´n file nh·∫°c (t√πy ch·ªçn)
    Returns:
        Dict ch·ª©a k·∫øt qu·∫£ t·∫°o video
    """
    try:
        # Global declarations for manual selection handling
        global IS_MANUAL_SELECTION, MANUAL_SELECTED_FILES
        
        print(f"\nüöÄ B·∫ÆT ƒê·∫¶U T·∫†O VIDEO CHO USER: {username}")
        print(f"üìÅ Th∆∞ m·ª•c: {user_folder}")
        print(f"üìê ƒê·ªãnh d·∫°ng: {video_format}")
        print(f"üìã S·ªë file: {len(selected_files)}")
        print(f"üéµ Nh·∫°c: {music_path if music_path else 'Kh√¥ng c√≥'}")
        
        # Set global flag to indicate this is manual selection (user chose specific files)
        IS_MANUAL_SELECTION = True
        
        # Set global selected files for the memory creation process
        MANUAL_SELECTED_FILES = selected_files
        # Prepare user music folder (for backward compatibility if needed)
        user_music_folder = os.path.join(os.getcwd(), 'input', username, 'music')
        os.makedirs(user_music_folder, exist_ok=True)
        
        # If music_path is a URL, download to user music folder
        if music_path and isinstance(music_path, str) and music_path.startswith(('http://', 'https://')):
            print(f"üåê Downloading custom music for user {username}...")
            downloaded_music = download_audio_from_url(music_path, user_music_folder)
            if downloaded_music:
                print(f"‚úÖ Custom music downloaded: {downloaded_music}")
                music_path = downloaded_music
            else:
                print(f"‚ùå Failed to download custom music, fallback to no music.")
                music_path = None
        # If music_path is already a valid file path, use it directly
        elif music_path and os.path.exists(music_path):
            print(f"‚úÖ Using existing music file: {music_path}")
        
        # Thi·∫øt l·∫≠p folder input cho module
        global INPUT_FOLDER
        INPUT_FOLDER = user_folder
        
        # Thi·∫øt l·∫≠p resolution theo format
        global RESOLUTION
        if video_format == 'vertical':
            RESOLUTION = RESOLUTION_VERTICAL
            print("üì± Ch·∫ø ƒë·ªô d·ªçc (9:16) - TikTok/Instagram Reels")
        elif video_format == 'square':
            RESOLUTION = RESOLUTION_SQUARE
            print("‚¨ú Ch·∫ø ƒë·ªô vu√¥ng (1:1) - Instagram Post")
        else:
            RESOLUTION = RESOLUTION_HORIZONTAL
            print("üì∫ Ch·∫ø ƒë·ªô ngang (16:9) - YouTube/TV")
        
        # Filter only selected files in the INPUT_FOLDER (scan subfolders)
        all_files = []
        supported_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.mp4', '.mov', '.avi']
        
        # Scan root folder
        if os.path.exists(INPUT_FOLDER):
            for f in os.listdir(INPUT_FOLDER):
                if os.path.splitext(f)[1].lower() in supported_extensions:
                    all_files.append(f)
        
        # Scan images subfolder
        images_folder = os.path.join(INPUT_FOLDER, 'images')
        if os.path.exists(images_folder):
            for f in os.listdir(images_folder):
                if os.path.splitext(f)[1].lower() in supported_extensions:
                    all_files.append(f)
        
        # Scan videos subfolder  
        videos_folder = os.path.join(INPUT_FOLDER, 'videos')
        if os.path.exists(videos_folder):
            for f in os.listdir(videos_folder):
                if os.path.splitext(f)[1].lower() in supported_extensions:
                    all_files.append(f)
        
        # DEBUG: Log file info ƒë·ªÉ t√¨m l·ªói
        import datetime
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        print(f"üîç DEBUG [{timestamp}]: INPUT_FOLDER = {INPUT_FOLDER}")
        print(f"üîç DEBUG [{timestamp}]: Total files in folder: {len(all_files)}")
        print(f"üîç DEBUG [{timestamp}]: Selected files count: {len(selected_files)}")
        print(f"üîç DEBUG [{timestamp}]: First 5 all_files: {all_files[:5]}")
        print(f"üîç DEBUG [{timestamp}]: First 5 selected_files: {selected_files[:5]}")
        
        available_files = [f for f in selected_files if f in all_files]
        
        print(f"üîç DEBUG [{timestamp}]: Available files count: {len(available_files)}")
        print(f"üîç DEBUG [{timestamp}]: Available files list: {available_files[:5]}")  # Show actual matches
        if not available_files and selected_files:
            print(f"üîç DEBUG [{timestamp}]: Checking why files not found...")
            for sel_file in selected_files[:3]:  # Check first 3
                if sel_file in all_files:
                    print(f"‚úÖ DEBUG [{timestamp}]: Found {sel_file}")
                else:
                    print(f"‚ùå DEBUG [{timestamp}]: Missing {sel_file}")
                    # Check if file exists with different case or characters
                    for all_file in all_files:
                        if sel_file.lower() == all_file.lower():
                            print(f"üîç DEBUG [{timestamp}]: Case mismatch: '{sel_file}' vs '{all_file}'")
                        elif sel_file in all_file or all_file in sel_file:
                            print(f"üîç DEBUG [{timestamp}]: Partial match: '{sel_file}' vs '{all_file}'")
        
        if not available_files:
            return {
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y file n√†o h·ª£p l·ªá trong danh s√°ch ƒë√£ ch·ªçn!',
                'error_code': 'NO_VALID_FILES',
                'debug_info': {
                    'input_folder': INPUT_FOLDER,
                    'total_files': len(all_files),
                    'selected_files': len(selected_files),
                    'available_files': len(available_files),
                    'first_selected': selected_files[:3] if selected_files else [],
                    'first_available': all_files[:3] if all_files else []
                }
            }
        
        print(f"‚úÖ T√¨m th·∫•y {len(available_files)} file h·ª£p l·ªá")
        
        # Create helper function to find file in subfolders
        def find_file_path(filename):
            """Find the full path of a file in INPUT_FOLDER or its subfolders"""
            # Try root folder first
            root_path = os.path.join(INPUT_FOLDER, filename)
            if os.path.exists(root_path):
                return root_path
            
            # Try images subfolder
            images_path = os.path.join(INPUT_FOLDER, 'images', filename)
            if os.path.exists(images_path):
                return images_path
            
            # Try videos subfolder
            videos_path = os.path.join(INPUT_FOLDER, 'videos', filename)
            if os.path.exists(videos_path):
                return videos_path
            
            # If not found anywhere, return original path for error handling
            return root_path
        
        # Store original INPUT_FOLDER as we'll need to override with subfolder logic
        global find_file_path_func
        find_file_path_func = find_file_path
        
        # T·∫°o th∆∞ m·ª•c memories cho user (ƒë√∫ng ƒë∆∞·ªùng d·∫´n)
        output_folder = os.path.join('E:', 'EverLiving_UserData', username, 'memories')
        if not os.path.exists(output_folder):
            os.makedirs(output_folder, exist_ok=True)
            print(f"üìÅ T·∫°o th∆∞ m·ª•c memories: {output_folder}")
        
        # Set OUTPUT_FOLDER cho module
        global OUTPUT_FOLDER
        original_output_folder = OUTPUT_FOLDER
        OUTPUT_FOLDER = output_folder
        print(f"üìÅ ƒê·∫∑t output folder: {OUTPUT_FOLDER}")
        print(f"üìÅ Original output folder: {original_output_folder}")
        
        # Temporarily backup original files list and set selected files
        original_files_backup = None
        try:
            # Save current state
            image_files = [f for f in available_files if os.path.splitext(f)[1].lower() in ['.jpg', '.jpeg', '.png', '.webp']]
            video_files = [f for f in available_files if os.path.splitext(f)[1].lower() in ['.mp4', '.mov', '.avi']]
            
            if not image_files and not video_files:
                return {
                    'success': False,
                    'error': 'Kh√¥ng c√≥ file ·∫£nh ho·∫∑c video h·ª£p l·ªá!',
                    'error_code': 'NO_MEDIA_FILES'
                }
            
            print(f"üìä Ph√¢n lo·∫°i: {len(image_files)} ·∫£nh, {len(video_files)} video")
            
            # Call the main video creation function with music
            start_time = time.time()
            create_memories_video(custom_music_path=music_path)
            processing_time = time.time() - start_time
            
            # Find the created video file (most recent .mp4 in output folder)
            video_files_in_folder = [f for f in os.listdir(output_folder) if f.endswith('.mp4')]
            
            if not video_files_in_folder:
                # Restore original output folder
                OUTPUT_FOLDER = original_output_folder
                return {
                    'success': False,
                    'error': f'Video ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω nh∆∞ng kh√¥ng t√¨m th·∫•y file output trong {output_folder}!',
                    'error_code': 'OUTPUT_NOT_FOUND'
                }
            
            # Get the most recent video file
            video_files_with_time = [(f, os.path.getmtime(os.path.join(output_folder, f))) for f in video_files_in_folder]
            video_files_with_time.sort(key=lambda x: x[1], reverse=True)
            latest_video = video_files_with_time[0][0]
            video_path = os.path.join(output_folder, latest_video)
            
            # Restore original output folder
            OUTPUT_FOLDER = original_output_folder
            
            print(f"‚úÖ Video t·∫°o th√†nh c√¥ng: {latest_video}")
            print(f"üìÅ L∆∞u trong: {output_folder}")
            print(f"‚è±Ô∏è Th·ªùi gian x·ª≠ l√Ω: {processing_time:.1f}s")
            
            # No temp cleanup needed, music is saved in user folder
            
            # Reset manual selection flags for next run
            IS_MANUAL_SELECTION = False
            MANUAL_SELECTED_FILES = []
            
            return {
                'success': True,
                'video_path': video_path,
                'filename': latest_video,
                'processing_time': processing_time,
                'frames_processed': len(available_files),
                'resolution': f"{RESOLUTION[0]}x{RESOLUTION[1]}",
                'format': video_format
            }
            
        except Exception as creation_error:
            # Restore original output folder
            OUTPUT_FOLDER = original_output_folder
            
            # Reset manual selection flags for next run
            IS_MANUAL_SELECTION = False
            MANUAL_SELECTED_FILES = []
            
            print(f"‚ùå L·ªói trong qu√° tr√¨nh t·∫°o video: {str(creation_error)}")
            return {
                'success': False,
                'error': f'L·ªói t·∫°o video: {str(creation_error)}',
                'error_code': 'CREATION_ERROR',
                'error_details': str(creation_error)
            }
            
    except Exception as e:
        # Restore original output folder if it was set
        try:
            if 'original_output_folder' in locals():
                OUTPUT_FOLDER = original_output_folder
        except:
            pass
        print(f"‚ùå L·ªñI NGHI√äM TR·ªåNG trong create_video_from_images: {str(e)}")
        return {
            'success': False,
            'error': f'L·ªói h·ªá th·ªëng: {str(e)}',
            'error_code': 'SYSTEM_ERROR',
            'error_details': str(e)
        }


def set_user_folder(folder_path):
    """Thi·∫øt l·∫≠p th∆∞ m·ª•c user cho video processor"""
    global INPUT_FOLDER
    INPUT_FOLDER = folder_path
    print(f"üìÅ ƒê√£ thi·∫øt l·∫≠p th∆∞ m·ª•c user: {folder_path}")


if __name__ == "__main__":
    try:
        log("=" * 60)
        log("KH·ªûI ƒê·ªòNG CH∆Ø∆†NG TR√åNH T·∫†O VIDEO K·ª∂ NI·ªÜM V·ªöI LOGIC M·ªöI")
        log("=" * 60)
        
        # ===== CH·ªåN ORIENTATION =====
        print("\nüé¨ CH·ªåN ƒê·ªäNH D·∫†NG VIDEO:")
        print("1. üì∫ Ngang (16:9) - 1280x720 - YouTube/TV")
        print("2. üì± D·ªçc (9:16) - 720x1280 - TikTok/Instagram Reels")
        print("3. ‚¨ú Vu√¥ng (1:1) - 720x720 - Instagram Post")
        
        while True:
            choice = input("\nNh·∫≠p l·ª±a ch·ªçn (1/2/3): ").strip()
            if choice == "1":
                globals()['RESOLUTION'] = RESOLUTION_HORIZONTAL
                orientation = "NGANG (16:9)"
                break
            elif choice == "2":
                globals()['RESOLUTION'] = RESOLUTION_VERTICAL
                orientation = "D·ªåC (9:16)"
                break
            elif choice == "3":
                globals()['RESOLUTION'] = RESOLUTION_SQUARE
                orientation = "VU√îNG (1:1)"
                break
            else:
                print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p 1, 2 ho·∫∑c 3.")
        
        log(f"‚úÖ ƒê√£ ch·ªçn ƒë·ªãnh d·∫°ng: {orientation} - {RESOLUTION[0]}x{RESOLUTION[1]}")
        
        create_memories_video()
    except Exception as e:
        log(f"‚ùå L·ªñI CH∆Ø∆†NG TR√åNH: {str(e)}")
        raise